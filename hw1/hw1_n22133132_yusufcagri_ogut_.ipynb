{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XI6G2xFdyn5"
      },
      "source": [
        "# Regression Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccLlnATTdyn8"
      },
      "source": [
        "In this assignment you will train a model for a regression problem based on linear regression with polynomial feature extension. You are not allowed to use sklearn or any existing python libraries that implements Linear or Polynomial Regresssion in any form. You are expected to write the source code by yourselves, including the gradient descent algorithm. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLNEre4Adyn9"
      },
      "source": [
        "## Dataset\n",
        "You are given a 1D dataset and the ground truth targets for the regression in train.csv file. You are also provided with test.csv file for the model evaluations. Do not use any data from the test data in your model trainings. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMz5J5uBdyn-"
      },
      "source": [
        "## Requirements\n",
        "Include all the source codes in the following cells; make sure that the codes are sufficiently documented in each section. After training your model, provide regression plots for the train and test data (separately) as well. Report your MSE and average Root Mean Square Error that you obtain using the test data in the Report Section (below). Also, provide all the hyperparameters and their values following the source codes in seperate cells."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect gdrive for train/test dataset:"
      ],
      "metadata": {
        "id": "1IuYoWzy4_SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive # import lib\n",
        "drive.mount(\"/content/gdrive\") # mount gdrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g5iE9aNd4OO",
        "outputId": "7c12c7f6-61c3-4a4f-b354-dfe53bb520e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries:"
      ],
      "metadata": {
        "id": "1fDlAJAu5HcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # for np array ops\n",
        "import matplotlib.pyplot as plt # for visualization purposes\n",
        "from pandas import * # for array ops"
      ],
      "metadata": {
        "id": "xr4Xjkxflw65"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read .csv from gdrive and parse \"x\" and \"t\" into two lists:"
      ],
      "metadata": {
        "id": "Aubzjodc5Ki0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sTrainAdd = \"/content/gdrive/MyDrive/cmp712/hw1/train.csv\" # address of train.csv on gdriveg\n",
        "sTestAdd  = \"/content/gdrive/MyDrive/cmp712/hw1/test.csv\" # address of test.csv on gdriveg\n",
        "data = read_csv(sTrainAdd) # read train.csv\n",
        "X = data['x'] # assign dataset argument into X\n",
        "vIdx = sorted(range(len(X)), key=lambda k: X[k])\n",
        "X = X[vIdx]\n",
        "min_x = min(X)\n",
        "X = X - min_x\n",
        "#X.sort_values(ascending=True)\n",
        "T = data['t'] # assign dataset value into T\n",
        "T = T[vIdx]\n",
        "TRAIN_DATASET_LENGTH = len(X) # get length of dataset\n",
        "\n",
        "data_test = read_csv(sTestAdd) # read train.csv\n",
        "X_test = data_test['x']\n",
        "vIdx_test = sorted(range(len(X_test)), key=lambda k: X_test[k])\n",
        "X_test = X_test[vIdx_test]\n",
        "min_x_test = min(X_test)\n",
        "X_test = X_test - min_x_test\n",
        "#X_test.sort_values(ascending=True)\n",
        "T_test = data_test['t'] # assign dataset value into T\n",
        "T_test = T_test[vIdx_test]\n",
        "TEST_DATASET_LENGTH = len(X_test) # get length of dataset\n",
        "\n",
        "\n",
        "print('Length of Training Data: ', TRAIN_DATASET_LENGTH)\n",
        "print('Train data visualized (x axis: X, y axis: T:')\n",
        "plt.scatter(X, T) # check if data is ok\n",
        "print('Length of Test Data: ', TEST_DATASET_LENGTH)\n",
        "print('Test data visualized (x axis: X, y axis: T:')\n",
        "plt.scatter(X_test, T_test) # check if data is ok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "qHtvwyZqeWOR",
        "outputId": "5099ef09-10b4-440f-a110-62a45f4db8ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of Training Data:  150\n",
            "Train data visualized (x axis: X, y axis: T:\n",
            "Length of Test Data:  100\n",
            "Test data visualized (x axis: X, y axis: T:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7efd6ea79b80>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**updateParams**, **calcDeriv** and **calcPoly** methods are created in order to set a generic structure for testing different order polynomials:"
      ],
      "metadata": {
        "id": "ANmtb1AbzEkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generic method to calculate partial derivatives of MSE loss function wrt to polynomial parameters:"
      ],
      "metadata": {
        "id": "mefXDM0jKfbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcLossPartialDeriv(T_Pred, nOrder):\n",
        "  res = (-2/TRAIN_DATASET_LENGTH) * np.sum((T - T_Pred) * np.transpose(np.power(X, nOrder)))\n",
        "  return res"
      ],
      "metadata": {
        "id": "z2XBDFDguFHn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generic method to calculate different order polynomials:"
      ],
      "metadata": {
        "id": "NACQFM25KaTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcPoly(x, params):\n",
        "  sum = 0\n",
        "  nOrder = len(params) - 1\n",
        "  for n in range(0, len(params)):\n",
        "    sum += params[n] * np.power(x, nOrder - n)\n",
        "  return sum"
      ],
      "metadata": {
        "id": "zzm5OEwdmTTA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function is Mean Squared Error **(MSE)**:"
      ],
      "metadata": {
        "id": "OJvZWMhszXIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcLoss(truth, pred, vParams, nReg):\n",
        "  loss = 1/TRAIN_DATASET_LENGTH * np.sum(np.power(truth - pred, 2)) + nReg * sum(vParams)/len(vParams)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "kcI9FLcVmFTc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer method:"
      ],
      "metadata": {
        "id": "uwHym-_4ze47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainer(X, T, nLearningRate, nPolyOrder, nLossLim, nIterLim, nReg):\n",
        "  vLoss_training = np.zeros(nIterLim)\n",
        "  vLoss_test = np.zeros(nIterLim)\n",
        "  vPolyParams = np.zeros(nPolyOrder + 1) # Polynomial constants stored in np array\n",
        "  vLossPartialDerivs = np.zeros(nPolyOrder + 1) # Partial derivatives of loss function stored in np array\n",
        "  nIter = 0 # Iteration counter\n",
        "  nLoss_training = 999999 # Used for exit condition of minimum loss, initially set to a large number\n",
        "  nLoss_test = 0\n",
        "  while (abs(nLoss_training)>nLossLim) and (nIter<nIterLim): # train until loss decreases to a certain value or number of iterations reached\n",
        "    T_Pred = calcPoly(X, vPolyParams) # calculate predictor's values\n",
        "    nLoss_training = calcLoss(T, T_Pred, vPolyParams, nReg) # calculate loss\n",
        "    vLoss_training[nIter] = nLoss_training\n",
        "    for i in range(0, nPolyOrder+1): # calculate derivatives and update parameters\n",
        "      vLossPartialDerivs[i] = calcLossPartialDeriv(T_Pred, nPolyOrder-i) # calculate partial derivatives\n",
        "    vPolyParams = vPolyParams - nLearningRate * vLossPartialDerivs\n",
        "    T_Pred_test = calcPoly(X_test, vPolyParams) # calculate predictor's values\n",
        "    nLoss_test = calcLoss(T_test, T_Pred_test, vPolyParams, nReg) # calculate loss\n",
        "    vLoss_test[nIter] = nLoss_test    \n",
        "    nIter = nIter + 1\n",
        "    if(nIter%100==0):\n",
        "        print(\"progress:\", nIter/nIterLim*100, \"Training Loss=\", nLoss_training)\n",
        "        print(\"progress:\", nIter/nIterLim*100, \"Test Loss=\", nLoss_test)\n",
        "  return vPolyParams, vLoss_training, vLoss_test"
      ],
      "metadata": {
        "id": "Ibb-10lTHJ1z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer wrapper:"
      ],
      "metadata": {
        "id": "I32pknUkHZLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparam.s:\n",
        "LEARNING_RATE = 0.1\n",
        "REGULARIZATION = 0 #0.2\n",
        "POLY_ORDER = 8\n",
        "LIM_LOSS = 0.000001\n",
        "LIM_ITER = 50000\n",
        "vPolyParams, vLoss_training, vLoss_test = trainer(X, T, LEARNING_RATE, POLY_ORDER, LIM_LOSS, LIM_ITER, REGULARIZATION)\n",
        "print(vPolyParams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQXAEhAXksZl",
        "outputId": "86602f57-1938-48f0-e3c6-c15a161650cc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "progress: 0.2 Training Loss= 0.1539373507925425\n",
            "progress: 0.2 Test Loss= 0.0858589392175831\n",
            "progress: 0.4 Training Loss= 0.15045114742398388\n",
            "progress: 0.4 Test Loss= 0.08697510198960401\n",
            "progress: 0.6 Training Loss= 0.14866847793117519\n",
            "progress: 0.6 Test Loss= 0.08785293301860224\n",
            "progress: 0.8 Training Loss= 0.14747130086449636\n",
            "progress: 0.8 Test Loss= 0.088327756034572\n",
            "progress: 1.0 Training Loss= 0.14650683065939496\n",
            "progress: 1.0 Test Loss= 0.0885072893780556\n",
            "progress: 1.2 Training Loss= 0.14566098592307172\n",
            "progress: 1.2 Test Loss= 0.08851048966546414\n",
            "progress: 1.4000000000000001 Training Loss= 0.14489454159113535\n",
            "progress: 1.4000000000000001 Test Loss= 0.08842106824935168\n",
            "progress: 1.6 Training Loss= 0.14419193280224749\n",
            "progress: 1.6 Test Loss= 0.08829022634915325\n",
            "progress: 1.7999999999999998 Training Loss= 0.14354524352714176\n",
            "progress: 1.7999999999999998 Test Loss= 0.08814710840630781\n",
            "progress: 2.0 Training Loss= 0.14294918566813247\n",
            "progress: 2.0 Test Loss= 0.08800740748057002\n",
            "progress: 2.1999999999999997 Training Loss= 0.1423995122424821\n",
            "progress: 2.1999999999999997 Test Loss= 0.08787902233318454\n",
            "progress: 2.4 Training Loss= 0.1418925045480387\n",
            "progress: 2.4 Test Loss= 0.08776547342925194\n",
            "progress: 2.6 Training Loss= 0.1414247961000465\n",
            "progress: 2.6 Test Loss= 0.08766788395681527\n",
            "progress: 2.8000000000000003 Training Loss= 0.14099330299098314\n",
            "progress: 2.8000000000000003 Test Loss= 0.0875861035419089\n",
            "progress: 3.0 Training Loss= 0.14059518863473927\n",
            "progress: 3.0 Test Loss= 0.08751933660183793\n",
            "progress: 3.2 Training Loss= 0.14022784030658805\n",
            "progress: 3.2 Test Loss= 0.08746648947820591\n",
            "progress: 3.4000000000000004 Training Loss= 0.13988885034146142\n",
            "progress: 3.4000000000000004 Test Loss= 0.08742635953472389\n",
            "progress: 3.5999999999999996 Training Loss= 0.13957599968513593\n",
            "progress: 3.5999999999999996 Test Loss= 0.08739773604392721\n",
            "progress: 3.8 Training Loss= 0.13928724300803244\n",
            "progress: 3.8 Test Loss= 0.08737945212006397\n",
            "progress: 4.0 Training Loss= 0.13902069507002532\n",
            "progress: 4.0 Test Loss= 0.08737040966445439\n",
            "progress: 4.2 Training Loss= 0.13877461817922895\n",
            "progress: 4.2 Test Loss= 0.087369589575133\n",
            "progress: 4.3999999999999995 Training Loss= 0.13854741064067924\n",
            "progress: 4.3999999999999995 Test Loss= 0.087376054035573\n",
            "progress: 4.6 Training Loss= 0.13833759611167035\n",
            "progress: 4.6 Test Loss= 0.08738894466114888\n",
            "progress: 4.8 Training Loss= 0.1381438137909892\n",
            "progress: 4.8 Test Loss= 0.08740747858909478\n",
            "progress: 5.0 Training Loss= 0.1379648093762215\n",
            "progress: 5.0 Test Loss= 0.08743094365499852\n",
            "progress: 5.2 Training Loss= 0.1377994267288426\n",
            "progress: 5.2 Test Loss= 0.0874586932746746\n",
            "progress: 5.4 Training Loss= 0.13764660019164515\n",
            "progress: 5.4 Test Loss= 0.08749014135940042\n",
            "progress: 5.6000000000000005 Training Loss= 0.13750534750743304\n",
            "progress: 5.6000000000000005 Test Loss= 0.08752475743165294\n",
            "progress: 5.800000000000001 Training Loss= 0.13737476329191914\n",
            "progress: 5.800000000000001 Test Loss= 0.08756206202003261\n",
            "progress: 6.0 Training Loss= 0.13725401301745213\n",
            "progress: 6.0 Test Loss= 0.08760162236388093\n",
            "progress: 6.2 Training Loss= 0.13714232746759275\n",
            "progress: 6.2 Test Loss= 0.08764304843228711\n",
            "progress: 6.4 Training Loss= 0.13703899762569\n",
            "progress: 6.4 Test Loss= 0.08768598924875727\n",
            "progress: 6.6000000000000005 Training Loss= 0.13694336996349019\n",
            "progress: 6.6000000000000005 Test Loss= 0.08773012950623145\n",
            "progress: 6.800000000000001 Training Loss= 0.13685484209847246\n",
            "progress: 6.800000000000001 Test Loss= 0.08777518645428734\n",
            "progress: 7.000000000000001 Training Loss= 0.13677285879105208\n",
            "progress: 7.000000000000001 Test Loss= 0.08782090703954233\n",
            "progress: 7.199999999999999 Training Loss= 0.13669690825505362\n",
            "progress: 7.199999999999999 Test Loss= 0.08786706528050833\n",
            "progress: 7.3999999999999995 Training Loss= 0.1366265187569358\n",
            "progress: 7.3999999999999995 Test Loss= 0.08791345985893409\n",
            "progress: 7.6 Training Loss= 0.13656125548117032\n",
            "progress: 7.6 Test Loss= 0.08795991191070338\n",
            "progress: 7.8 Training Loss= 0.13650071764094382\n",
            "progress: 7.8 Test Loss= 0.08800626300048005\n",
            "progress: 8.0 Training Loss= 0.1364445358149847\n",
            "progress: 8.0 Test Loss= 0.08805237326542653\n",
            "progress: 8.200000000000001 Training Loss= 0.13639236949281605\n",
            "progress: 8.200000000000001 Test Loss= 0.08809811971441794\n",
            "progress: 8.4 Training Loss= 0.13634390481212397\n",
            "progress: 8.4 Test Loss= 0.0881433946702172\n",
            "progress: 8.6 Training Loss= 0.13629885247320556\n",
            "progress: 8.6 Test Loss= 0.08818810434305241\n",
            "progress: 8.799999999999999 Training Loss= 0.13625694581663692\n",
            "progress: 8.799999999999999 Test Loss= 0.08823216752494573\n",
            "progress: 9.0 Training Loss= 0.1362179390513891\n",
            "progress: 9.0 Test Loss= 0.0882755143949863\n",
            "progress: 9.2 Training Loss= 0.13618160562161508\n",
            "progress: 9.2 Test Loss= 0.08831808542651746\n",
            "progress: 9.4 Training Loss= 0.13614773670125718\n",
            "progress: 9.4 Test Loss= 0.08835983038792714\n",
            "progress: 9.6 Training Loss= 0.13611613980647005\n",
            "progress: 9.6 Test Loss= 0.08840070742939392\n",
            "progress: 9.8 Training Loss= 0.13608663751663921\n",
            "progress: 9.8 Test Loss= 0.08844068224855155\n",
            "progress: 10.0 Training Loss= 0.13605906629549597\n",
            "progress: 10.0 Test Loss= 0.08847972732859696\n",
            "progress: 10.2 Training Loss= 0.13603327540449492\n",
            "progress: 10.2 Test Loss= 0.08851782124288708\n",
            "progress: 10.4 Training Loss= 0.13600912590123368\n",
            "progress: 10.4 Test Loss= 0.08855494802054505\n",
            "progress: 10.6 Training Loss= 0.13598648971625843\n",
            "progress: 10.6 Test Loss= 0.08859109656803772\n",
            "progress: 10.8 Training Loss= 0.1359652488021215\n",
            "progress: 10.8 Test Loss= 0.08862626014209077\n",
            "progress: 11.0 Training Loss= 0.13594529434903574\n",
            "progress: 11.0 Test Loss= 0.08866043586967987\n",
            "progress: 11.200000000000001 Training Loss= 0.13592652606191388\n",
            "progress: 11.200000000000001 Test Loss= 0.08869362431118091\n",
            "progress: 11.4 Training Loss= 0.13590885149398857\n",
            "progress: 11.4 Test Loss= 0.08872582906307697\n",
            "progress: 11.600000000000001 Training Loss= 0.13589218543258536\n",
            "progress: 11.600000000000001 Test Loss= 0.08875705639691103\n",
            "progress: 11.799999999999999 Training Loss= 0.13587644933296633\n",
            "progress: 11.799999999999999 Test Loss= 0.08878731493144182\n",
            "progress: 12.0 Training Loss= 0.13586157079648276\n",
            "progress: 12.0 Test Loss= 0.0888166153352053\n",
            "progress: 12.2 Training Loss= 0.1358474830895691\n",
            "progress: 12.2 Test Loss= 0.08884497005691223\n",
            "progress: 12.4 Training Loss= 0.13583412470038145\n",
            "progress: 12.4 Test Loss= 0.08887239308132115\n",
            "progress: 12.6 Training Loss= 0.13582143893013476\n",
            "progress: 12.6 Test Loss= 0.08889889970841625\n",
            "progress: 12.8 Training Loss= 0.13580937351642314\n",
            "progress: 12.8 Test Loss= 0.0889245063538997\n",
            "progress: 13.0 Training Loss= 0.13579788028602005\n",
            "progress: 13.0 Test Loss= 0.08894923036916745\n",
            "progress: 13.200000000000001 Training Loss= 0.1357869148348513\n",
            "progress: 13.200000000000001 Test Loss= 0.08897308987908908\n",
            "progress: 13.4 Training Loss= 0.13577643623301386\n",
            "progress: 13.4 Test Loss= 0.0889961036360494\n",
            "progress: 13.600000000000001 Training Loss= 0.1357664067528814\n",
            "progress: 13.600000000000001 Test Loss= 0.08901829088883635\n",
            "progress: 13.8 Training Loss= 0.13575679161848805\n",
            "progress: 13.8 Test Loss= 0.08903967126507586\n",
            "progress: 14.000000000000002 Training Loss= 0.13574755877452704\n",
            "progress: 14.000000000000002 Test Loss= 0.08906026466602167\n",
            "progress: 14.2 Training Loss= 0.1357386786734271\n",
            "progress: 14.2 Test Loss= 0.08908009117260744\n",
            "progress: 14.399999999999999 Training Loss= 0.13573012407909368\n",
            "progress: 14.399999999999999 Test Loss= 0.08909917096175758\n",
            "progress: 14.6 Training Loss= 0.1357218698860094\n",
            "progress: 14.6 Test Loss= 0.08911752423203803\n",
            "progress: 14.799999999999999 Training Loss= 0.1357138929524922\n",
            "progress: 14.799999999999999 Test Loss= 0.08913517113780392\n",
            "progress: 15.0 Training Loss= 0.13570617194700335\n",
            "progress: 15.0 Test Loss= 0.08915213173107177\n",
            "progress: 15.2 Training Loss= 0.13569868720648368\n",
            "progress: 15.2 Test Loss= 0.08916842591040916\n",
            "progress: 15.4 Training Loss= 0.13569142060577657\n",
            "progress: 15.4 Test Loss= 0.089184073376193\n",
            "progress: 15.6 Training Loss= 0.13568435543727073\n",
            "progress: 15.6 Test Loss= 0.08919909359164366\n",
            "progress: 15.8 Training Loss= 0.13567747629996188\n",
            "progress: 15.8 Test Loss= 0.08921350574909152\n",
            "progress: 16.0 Training Loss= 0.13567076899719713\n",
            "progress: 16.0 Test Loss= 0.08922732874097905\n",
            "progress: 16.2 Training Loss= 0.13566422044242202\n",
            "progress: 16.2 Test Loss= 0.08924058113514373\n",
            "progress: 16.400000000000002 Training Loss= 0.1356578185723041\n",
            "progress: 16.400000000000002 Test Loss= 0.08925328115396645\n",
            "progress: 16.6 Training Loss= 0.1356515522666556\n",
            "progress: 16.6 Test Loss= 0.08926544665700516\n",
            "progress: 16.8 Training Loss= 0.1356454112746229\n",
            "progress: 16.8 Test Loss= 0.08927709512676678\n",
            "progress: 17.0 Training Loss= 0.1356393861466534\n",
            "progress: 17.0 Test Loss= 0.08928824365730074\n",
            "progress: 17.2 Training Loss= 0.13563346817178604\n",
            "progress: 17.2 Test Loss= 0.08929890894532486\n",
            "progress: 17.4 Training Loss= 0.13562764931984958\n",
            "progress: 17.4 Test Loss= 0.08930910728361899\n",
            "progress: 17.599999999999998 Training Loss= 0.13562192218818558\n",
            "progress: 17.599999999999998 Test Loss= 0.08931885455644697\n",
            "progress: 17.8 Training Loss= 0.13561627995253966\n",
            "progress: 17.8 Test Loss= 0.08932816623678676\n",
            "progress: 18.0 Training Loss= 0.13561071632179714\n",
            "progress: 18.0 Test Loss= 0.0893370573851695\n",
            "progress: 18.2 Training Loss= 0.13560522549626067\n",
            "progress: 18.2 Test Loss= 0.08934554264994579\n",
            "progress: 18.4 Training Loss= 0.13559980212919331\n",
            "progress: 18.4 Test Loss= 0.08935363626881379\n",
            "progress: 18.6 Training Loss= 0.13559444129137124\n",
            "progress: 18.6 Test Loss= 0.0893613520714591\n",
            "progress: 18.8 Training Loss= 0.13558913843841097\n",
            "progress: 18.8 Test Loss= 0.08936870348317007\n",
            "progress: 19.0 Training Loss= 0.13558388938065308\n",
            "progress: 19.0 Test Loss= 0.08937570352930481\n",
            "progress: 19.2 Training Loss= 0.13557869025540337\n",
            "progress: 19.2 Test Loss= 0.08938236484049783\n",
            "progress: 19.400000000000002 Training Loss= 0.13557353750134643\n",
            "progress: 19.400000000000002 Test Loss= 0.08938869965850456\n",
            "progress: 19.6 Training Loss= 0.13556842783496134\n",
            "progress: 19.6 Test Loss= 0.08939471984259233\n",
            "progress: 19.8 Training Loss= 0.13556335822878351\n",
            "progress: 19.8 Test Loss= 0.0894004368763944\n",
            "progress: 20.0 Training Loss= 0.13555832589136765\n",
            "progress: 20.0 Test Loss= 0.08940586187515263\n",
            "progress: 20.200000000000003 Training Loss= 0.13555332824881897\n",
            "progress: 20.200000000000003 Test Loss= 0.08941100559328088\n",
            "progress: 20.4 Training Loss= 0.1355483629277693\n",
            "progress: 20.4 Test Loss= 0.08941587843218862\n",
            "progress: 20.599999999999998 Training Loss= 0.13554342773968653\n",
            "progress: 20.599999999999998 Test Loss= 0.08942049044831066\n",
            "progress: 20.8 Training Loss= 0.1355385206664109\n",
            "progress: 20.8 Test Loss= 0.08942485136129323\n",
            "progress: 21.0 Training Loss= 0.13553363984682415\n",
            "progress: 21.0 Test Loss= 0.08942897056229365\n",
            "progress: 21.2 Training Loss= 0.13552878356456144\n",
            "progress: 21.2 Test Loss= 0.0894328571223537\n",
            "progress: 21.4 Training Loss= 0.13552395023668515\n",
            "progress: 21.4 Test Loss= 0.08943651980081245\n",
            "progress: 21.6 Training Loss= 0.13551913840324506\n",
            "progress: 21.6 Test Loss= 0.08943996705372746\n",
            "progress: 21.8 Training Loss= 0.13551434671765558\n",
            "progress: 21.8 Test Loss= 0.08944320704227682\n",
            "progress: 22.0 Training Loss= 0.13550957393782617\n",
            "progress: 22.0 Test Loss= 0.08944624764111823\n",
            "progress: 22.2 Training Loss= 0.13550481891798558\n",
            "progress: 22.2 Test Loss= 0.08944909644668357\n",
            "progress: 22.400000000000002 Training Loss= 0.1355000806011465\n",
            "progress: 22.400000000000002 Test Loss= 0.08945176078539047\n",
            "progress: 22.6 Training Loss= 0.13549535801215926\n",
            "progress: 22.6 Test Loss= 0.08945424772175467\n",
            "progress: 22.8 Training Loss= 0.1354906502513098\n",
            "progress: 22.8 Test Loss= 0.08945656406638927\n",
            "progress: 23.0 Training Loss= 0.13548595648841816\n",
            "progress: 23.0 Test Loss= 0.08945871638387871\n",
            "progress: 23.200000000000003 Training Loss= 0.13548127595739934\n",
            "progress: 23.200000000000003 Test Loss= 0.0894607110005171\n",
            "progress: 23.400000000000002 Training Loss= 0.13547660795124913\n",
            "progress: 23.400000000000002 Test Loss= 0.0894625540119027\n",
            "progress: 23.599999999999998 Training Loss= 0.13547195181742316\n",
            "progress: 23.599999999999998 Test Loss= 0.0894642512903806\n",
            "progress: 23.799999999999997 Training Loss= 0.13546730695357706\n",
            "progress: 23.799999999999997 Test Loss= 0.08946580849232853\n",
            "progress: 24.0 Training Loss= 0.13546267280363997\n",
            "progress: 24.0 Test Loss= 0.08946723106528057\n",
            "progress: 24.2 Training Loss= 0.13545804885419585\n",
            "progress: 24.2 Test Loss= 0.08946852425488486\n",
            "progress: 24.4 Training Loss= 0.13545343463114742\n",
            "progress: 24.4 Test Loss= 0.08946969311169325\n",
            "progress: 24.6 Training Loss= 0.13544882969664143\n",
            "progress: 24.6 Test Loss= 0.08947074249778075\n",
            "progress: 24.8 Training Loss= 0.13544423364623467\n",
            "progress: 24.8 Test Loss= 0.08947167709319304\n",
            "progress: 25.0 Training Loss= 0.13543964610628154\n",
            "progress: 25.0 Test Loss= 0.08947250140222249\n",
            "progress: 25.2 Training Loss= 0.13543506673152608\n",
            "progress: 25.2 Test Loss= 0.08947321975951222\n",
            "progress: 25.4 Training Loss= 0.13543049520288286\n",
            "progress: 25.4 Test Loss= 0.08947383633598928\n",
            "progress: 25.6 Training Loss= 0.1354259312253908\n",
            "progress: 25.6 Test Loss= 0.08947435514462722\n",
            "progress: 25.8 Training Loss= 0.13542137452632796\n",
            "progress: 25.8 Test Loss= 0.08947478004604081\n",
            "progress: 26.0 Training Loss= 0.1354168248534731\n",
            "progress: 26.0 Test Loss= 0.08947511475391365\n",
            "progress: 26.200000000000003 Training Loss= 0.13541228197350375\n",
            "progress: 26.200000000000003 Test Loss= 0.08947536284026161\n",
            "progress: 26.400000000000002 Training Loss= 0.13540774567051964\n",
            "progress: 26.400000000000002 Test Loss= 0.08947552774053405\n",
            "progress: 26.6 Training Loss= 0.13540321574468142\n",
            "progress: 26.6 Test Loss= 0.0894756127585556\n",
            "progress: 26.8 Training Loss= 0.1353986920109561\n",
            "progress: 26.8 Test Loss= 0.08947562107131188\n",
            "progress: 27.0 Training Loss= 0.13539417429796072\n",
            "progress: 27.0 Test Loss= 0.08947555573358126\n",
            "progress: 27.200000000000003 Training Loss= 0.13538966244689615\n",
            "progress: 27.200000000000003 Test Loss= 0.08947541968241648\n",
            "progress: 27.400000000000002 Training Loss= 0.13538515631056502\n",
            "progress: 27.400000000000002 Test Loss= 0.08947521574147928\n",
            "progress: 27.6 Training Loss= 0.13538065575246575\n",
            "progress: 27.6 Test Loss= 0.08947494662523124\n",
            "progress: 27.800000000000004 Training Loss= 0.13537616064595778\n",
            "progress: 27.800000000000004 Test Loss= 0.08947461494298434\n",
            "progress: 28.000000000000004 Training Loss= 0.13537167087349247\n",
            "progress: 28.000000000000004 Test Loss= 0.08947422320281474\n",
            "progress: 28.199999999999996 Training Loss= 0.1353671863259036\n",
            "progress: 28.199999999999996 Test Loss= 0.08947377381534308\n",
            "progress: 28.4 Training Loss= 0.13536270690175373\n",
            "progress: 28.4 Test Loss= 0.08947326909738508\n",
            "progress: 28.599999999999998 Training Loss= 0.13535823250673149\n",
            "progress: 28.599999999999998 Test Loss= 0.08947271127547572\n",
            "progress: 28.799999999999997 Training Loss= 0.13535376305309646\n",
            "progress: 28.799999999999997 Test Loss= 0.08947210248927073\n",
            "progress: 28.999999999999996 Training Loss= 0.13534929845916713\n",
            "progress: 28.999999999999996 Test Loss= 0.08947144479482848\n",
            "progress: 29.2 Training Loss= 0.13534483864884878\n",
            "progress: 29.2 Test Loss= 0.0894707401677761\n",
            "progress: 29.4 Training Loss= 0.13534038355119876\n",
            "progress: 29.4 Test Loss= 0.08946999050636344\n",
            "progress: 29.599999999999998 Training Loss= 0.13533593310002554\n",
            "progress: 29.599999999999998 Test Loss= 0.0894691976344074\n",
            "progress: 29.799999999999997 Training Loss= 0.13533148723351895\n",
            "progress: 29.799999999999997 Test Loss= 0.0894683633041309\n",
            "progress: 30.0 Training Loss= 0.13532704589390976\n",
            "progress: 30.0 Test Loss= 0.08946748919889917\n",
            "progress: 30.2 Training Loss= 0.13532260902715565\n",
            "progress: 30.2 Test Loss= 0.08946657693585698\n",
            "progress: 30.4 Training Loss= 0.13531817658265166\n",
            "progress: 30.4 Test Loss= 0.08946562806846964\n",
            "progress: 30.599999999999998 Training Loss= 0.1353137485129636\n",
            "progress: 30.599999999999998 Test Loss= 0.089464644088971\n",
            "progress: 30.8 Training Loss= 0.13530932477358185\n",
            "progress: 30.8 Test Loss= 0.08946362643072162\n",
            "progress: 31.0 Training Loss= 0.13530490532269507\n",
            "progress: 31.0 Test Loss= 0.08946257647047984\n",
            "progress: 31.2 Training Loss= 0.13530049012098108\n",
            "progress: 31.2 Test Loss= 0.08946149553058887\n",
            "progress: 31.4 Training Loss= 0.13529607913141412\n",
            "progress: 31.4 Test Loss= 0.0894603848810825\n",
            "progress: 31.6 Training Loss= 0.13529167231908756\n",
            "progress: 31.6 Test Loss= 0.08945924574171246\n",
            "progress: 31.8 Training Loss= 0.13528726965105028\n",
            "progress: 31.8 Test Loss= 0.08945807928389991\n",
            "progress: 32.0 Training Loss= 0.13528287109615558\n",
            "progress: 32.0 Test Loss= 0.08945688663261359\n",
            "progress: 32.2 Training Loss= 0.13527847662492246\n",
            "progress: 32.2 Test Loss= 0.08945566886817745\n",
            "progress: 32.4 Training Loss= 0.1352740862094071\n",
            "progress: 32.4 Test Loss= 0.08945442702801026\n",
            "progress: 32.6 Training Loss= 0.13526969982308515\n",
            "progress: 32.6 Test Loss= 0.08945316210829905\n",
            "progress: 32.800000000000004 Training Loss= 0.13526531744074247\n",
            "progress: 32.800000000000004 Test Loss= 0.0894518750656094\n",
            "progress: 33.0 Training Loss= 0.13526093903837488\n",
            "progress: 33.0 Test Loss= 0.08945056681843448\n",
            "progress: 33.2 Training Loss= 0.1352565645930957\n",
            "progress: 33.2 Test Loss= 0.08944923824868492\n",
            "progress: 33.4 Training Loss= 0.13525219408305034\n",
            "progress: 33.4 Test Loss= 0.08944789020312188\n",
            "progress: 33.6 Training Loss= 0.1352478274873378\n",
            "progress: 33.6 Test Loss= 0.08944652349473567\n",
            "progress: 33.800000000000004 Training Loss= 0.13524346478593796\n",
            "progress: 33.800000000000004 Test Loss= 0.08944513890407081\n",
            "progress: 34.0 Training Loss= 0.13523910595964514\n",
            "progress: 34.0 Test Loss= 0.08944373718050094\n",
            "progress: 34.2 Training Loss= 0.13523475099000604\n",
            "progress: 34.2 Test Loss= 0.08944231904345412\n",
            "progress: 34.4 Training Loss= 0.1352303998592634\n",
            "progress: 34.4 Test Loss= 0.08944088518359132\n",
            "progress: 34.599999999999994 Training Loss= 0.13522605255030343\n",
            "progress: 34.599999999999994 Test Loss= 0.0894394362639391\n",
            "progress: 34.8 Training Loss= 0.13522170904660757\n",
            "progress: 34.8 Test Loss= 0.0894379729209789\n",
            "progress: 35.0 Training Loss= 0.1352173693322081\n",
            "progress: 35.0 Test Loss= 0.08943649576569354\n",
            "progress: 35.199999999999996 Training Loss= 0.13521303339164703\n",
            "progress: 35.199999999999996 Test Loss= 0.08943500538457387\n",
            "progress: 35.4 Training Loss= 0.13520870120993847\n",
            "progress: 35.4 Test Loss= 0.08943350234058578\n",
            "progress: 35.6 Training Loss= 0.1352043727725335\n",
            "progress: 35.6 Test Loss= 0.08943198717409993\n",
            "progress: 35.8 Training Loss= 0.1352000480652883\n",
            "progress: 35.8 Test Loss= 0.08943046040378513\n",
            "progress: 36.0 Training Loss= 0.1351957270744345\n",
            "progress: 36.0 Test Loss= 0.08942892252746695\n",
            "progress: 36.199999999999996 Training Loss= 0.13519140978655175\n",
            "progress: 36.199999999999996 Test Loss= 0.08942737402295288\n",
            "progress: 36.4 Training Loss= 0.13518709618854263\n",
            "progress: 36.4 Test Loss= 0.08942581534882521\n",
            "progress: 36.6 Training Loss= 0.1351827862676094\n",
            "progress: 36.6 Test Loss= 0.08942424694520287\n",
            "progress: 36.8 Training Loss= 0.1351784800112327\n",
            "progress: 36.8 Test Loss= 0.08942266923447349\n",
            "progress: 37.0 Training Loss= 0.13517417740715162\n",
            "progress: 37.0 Test Loss= 0.08942108262199694\n",
            "progress: 37.2 Training Loss= 0.13516987844334574\n",
            "progress: 37.2 Test Loss= 0.08941948749678097\n",
            "progress: 37.4 Training Loss= 0.13516558310801816\n",
            "progress: 37.4 Test Loss= 0.08941788423213054\n",
            "progress: 37.6 Training Loss= 0.13516129138958016\n",
            "progress: 37.6 Test Loss= 0.08941627318627159\n",
            "progress: 37.8 Training Loss= 0.135157003276637\n",
            "progress: 37.8 Test Loss= 0.08941465470295035\n",
            "progress: 38.0 Training Loss= 0.13515271875797444\n",
            "progress: 38.0 Test Loss= 0.08941302911200896\n",
            "progress: 38.2 Training Loss= 0.13514843782254696\n",
            "progress: 38.2 Test Loss= 0.08941139672993849\n",
            "progress: 38.4 Training Loss= 0.1351441604594665\n",
            "progress: 38.4 Test Loss= 0.08940975786041015\n",
            "progress: 38.6 Training Loss= 0.135139886657992\n",
            "progress: 38.6 Test Loss= 0.08940811279478572\n",
            "progress: 38.800000000000004 Training Loss= 0.13513561640752003\n",
            "progress: 38.800000000000004 Test Loss= 0.08940646181260743\n",
            "progress: 39.0 Training Loss= 0.13513134969757595\n",
            "progress: 39.0 Test Loss= 0.08940480518206899\n",
            "progress: 39.2 Training Loss= 0.1351270865178058\n",
            "progress: 39.2 Test Loss= 0.08940314316046763\n",
            "progress: 39.4 Training Loss= 0.13512282685796895\n",
            "progress: 39.4 Test Loss= 0.08940147599463855\n",
            "progress: 39.6 Training Loss= 0.13511857070793099\n",
            "progress: 39.6 Test Loss= 0.08939980392137198\n",
            "progress: 39.800000000000004 Training Loss= 0.13511431805765767\n",
            "progress: 39.800000000000004 Test Loss= 0.0893981271678139\n",
            "progress: 40.0 Training Loss= 0.13511006889720875\n",
            "progress: 40.0 Test Loss= 0.08939644595185069\n",
            "progress: 40.2 Training Loss= 0.13510582321673287\n",
            "progress: 40.2 Test Loss= 0.08939476048247887\n",
            "progress: 40.400000000000006 Training Loss= 0.13510158100646222\n",
            "progress: 40.400000000000006 Test Loss= 0.08939307096015983\n",
            "progress: 40.6 Training Loss= 0.13509734225670822\n",
            "progress: 40.6 Test Loss= 0.08939137757716079\n",
            "progress: 40.8 Training Loss= 0.13509310695785728\n",
            "progress: 40.8 Test Loss= 0.08938968051788192\n",
            "progress: 41.0 Training Loss= 0.1350888751003666\n",
            "progress: 41.0 Test Loss= 0.08938797995917093\n",
            "progress: 41.199999999999996 Training Loss= 0.13508464667476072\n",
            "progress: 41.199999999999996 Test Loss= 0.08938627607062472\n",
            "progress: 41.4 Training Loss= 0.13508042167162831\n",
            "progress: 41.4 Test Loss= 0.08938456901487936\n",
            "progress: 41.6 Training Loss= 0.1350762000816189\n",
            "progress: 41.6 Test Loss= 0.08938285894788821\n",
            "progress: 41.8 Training Loss= 0.13507198189544004\n",
            "progress: 41.8 Test Loss= 0.08938114601918962\n",
            "progress: 42.0 Training Loss= 0.13506776710385457\n",
            "progress: 42.0 Test Loss= 0.08937943037216323\n",
            "progress: 42.199999999999996 Training Loss= 0.13506355569767858\n",
            "progress: 42.199999999999996 Test Loss= 0.08937771214427656\n",
            "progress: 42.4 Training Loss= 0.13505934766777866\n",
            "progress: 42.4 Test Loss= 0.08937599146732171\n",
            "progress: 42.6 Training Loss= 0.1350551430050702\n",
            "progress: 42.6 Test Loss= 0.08937426846764249\n",
            "progress: 42.8 Training Loss= 0.13505094170051535\n",
            "progress: 42.8 Test Loss= 0.08937254326635263\n",
            "progress: 43.0 Training Loss= 0.13504674374512105\n",
            "progress: 43.0 Test Loss= 0.08937081597954548\n",
            "progress: 43.2 Training Loss= 0.1350425491299377\n",
            "progress: 43.2 Test Loss= 0.08936908671849496\n",
            "progress: 43.4 Training Loss= 0.1350383578460575\n",
            "progress: 43.4 Test Loss= 0.08936735558984873\n",
            "progress: 43.6 Training Loss= 0.135034169884613\n",
            "progress: 43.6 Test Loss= 0.0893656226958137\n",
            "progress: 43.8 Training Loss= 0.13502998523677584\n",
            "progress: 43.8 Test Loss= 0.0893638881343342\n",
            "progress: 44.0 Training Loss= 0.13502580389375554\n",
            "progress: 44.0 Test Loss= 0.08936215199926273\n",
            "progress: 44.2 Training Loss= 0.13502162584679853\n",
            "progress: 44.2 Test Loss= 0.08936041438052429\n",
            "progress: 44.4 Training Loss= 0.13501745108718677\n",
            "progress: 44.4 Test Loss= 0.08935867536427393\n",
            "progress: 44.6 Training Loss= 0.13501327960623716\n",
            "progress: 44.6 Test Loss= 0.08935693503304805\n",
            "progress: 44.800000000000004 Training Loss= 0.1350091113953004\n",
            "progress: 44.800000000000004 Test Loss= 0.08935519346590971\n",
            "progress: 45.0 Training Loss= 0.1350049464457604\n",
            "progress: 45.0 Test Loss= 0.08935345073858807\n",
            "progress: 45.2 Training Loss= 0.1350007847490332\n",
            "progress: 45.2 Test Loss= 0.08935170692361243\n",
            "progress: 45.4 Training Loss= 0.1349966262965666\n",
            "progress: 45.4 Test Loss= 0.08934996209044063\n",
            "progress: 45.6 Training Loss= 0.13499247107983925\n",
            "progress: 45.6 Test Loss= 0.08934821630558268\n",
            "progress: 45.800000000000004 Training Loss= 0.1349883190903602\n",
            "progress: 45.800000000000004 Test Loss= 0.08934646963271914\n",
            "progress: 46.0 Training Loss= 0.13498417031966808\n",
            "progress: 46.0 Test Loss= 0.08934472213281493\n",
            "progress: 46.2 Training Loss= 0.13498002475933102\n",
            "progress: 46.2 Test Loss= 0.08934297386422865\n",
            "progress: 46.400000000000006 Training Loss= 0.13497588240094563\n",
            "progress: 46.400000000000006 Test Loss= 0.08934122488281729\n",
            "progress: 46.6 Training Loss= 0.13497174323613692\n",
            "progress: 46.6 Test Loss= 0.08933947524203707\n",
            "progress: 46.800000000000004 Training Loss= 0.13496760725655785\n",
            "progress: 46.800000000000004 Test Loss= 0.08933772499304003\n",
            "progress: 47.0 Training Loss= 0.1349634744538885\n",
            "progress: 47.0 Test Loss= 0.08933597418476687\n",
            "progress: 47.199999999999996 Training Loss= 0.13495934481983635\n",
            "progress: 47.199999999999996 Test Loss= 0.08933422286403589\n",
            "progress: 47.4 Training Loss= 0.13495521834613539\n",
            "progress: 47.4 Test Loss= 0.0893324710756287\n",
            "progress: 47.599999999999994 Training Loss= 0.13495109502454614\n",
            "progress: 47.599999999999994 Test Loss= 0.08933071886237227\n",
            "progress: 47.8 Training Loss= 0.13494697484685506\n",
            "progress: 47.8 Test Loss= 0.0893289662652178\n",
            "progress: 48.0 Training Loss= 0.13494285780487447\n",
            "progress: 48.0 Test Loss= 0.08932721332331618\n",
            "progress: 48.199999999999996 Training Loss= 0.13493874389044228\n",
            "progress: 48.199999999999996 Test Loss= 0.08932546007409088\n",
            "progress: 48.4 Training Loss= 0.13493463309542156\n",
            "progress: 48.4 Test Loss= 0.08932370655330767\n",
            "progress: 48.6 Training Loss= 0.13493052541170047\n",
            "progress: 48.6 Test Loss= 0.08932195279514135\n",
            "progress: 48.8 Training Loss= 0.13492642083119194\n",
            "progress: 48.8 Test Loss= 0.08932019883224028\n",
            "progress: 49.0 Training Loss= 0.13492231934583346\n",
            "progress: 49.0 Test Loss= 0.08931844469578795\n",
            "progress: 49.2 Training Loss= 0.13491822094758707\n",
            "progress: 49.2 Test Loss= 0.08931669041556227\n",
            "progress: 49.4 Training Loss= 0.13491412562843874\n",
            "progress: 49.4 Test Loss= 0.08931493601999234\n",
            "progress: 49.6 Training Loss= 0.13491003338039873\n",
            "progress: 49.6 Test Loss= 0.08931318153621312\n",
            "progress: 49.8 Training Loss= 0.13490594419550095\n",
            "progress: 49.8 Test Loss= 0.0893114269901178\n",
            "progress: 50.0 Training Loss= 0.13490185806580315\n",
            "progress: 50.0 Test Loss= 0.08930967240640814\n",
            "progress: 50.2 Training Loss= 0.13489777498338645\n",
            "progress: 50.2 Test Loss= 0.08930791780864272\n",
            "progress: 50.4 Training Loss= 0.13489369494035544\n",
            "progress: 50.4 Test Loss= 0.08930616321928324\n",
            "progress: 50.6 Training Loss= 0.13488961792883794\n",
            "progress: 50.6 Test Loss= 0.08930440865973915\n",
            "progress: 50.8 Training Loss= 0.13488554394098493\n",
            "progress: 50.8 Test Loss= 0.08930265415041035\n",
            "progress: 51.0 Training Loss= 0.1348814729689703\n",
            "progress: 51.0 Test Loss= 0.089300899710728\n",
            "progress: 51.2 Training Loss= 0.13487740500499076\n",
            "progress: 51.2 Test Loss= 0.08929914535919417\n",
            "progress: 51.4 Training Loss= 0.13487334004126586\n",
            "progress: 51.4 Test Loss= 0.08929739111341947\n",
            "progress: 51.6 Training Loss= 0.13486927807003768\n",
            "progress: 51.6 Test Loss= 0.08929563699015924\n",
            "progress: 51.800000000000004 Training Loss= 0.13486521908357088\n",
            "progress: 51.800000000000004 Test Loss= 0.08929388300534863\n",
            "progress: 52.0 Training Loss= 0.1348611630741525\n",
            "progress: 52.0 Test Loss= 0.08929212917413573\n",
            "progress: 52.2 Training Loss= 0.13485711003409184\n",
            "progress: 52.2 Test Loss= 0.08929037551091393\n",
            "progress: 52.400000000000006 Training Loss= 0.1348530599557205\n",
            "progress: 52.400000000000006 Test Loss= 0.08928862202935245\n",
            "progress: 52.6 Training Loss= 0.13484901283139208\n",
            "progress: 52.6 Test Loss= 0.08928686874242625\n",
            "progress: 52.800000000000004 Training Loss= 0.1348449686534823\n",
            "progress: 52.800000000000004 Test Loss= 0.08928511566244395\n",
            "progress: 53.0 Training Loss= 0.13484092741438874\n",
            "progress: 53.0 Test Loss= 0.08928336280107547\n",
            "progress: 53.2 Training Loss= 0.13483688910653083\n",
            "progress: 53.2 Test Loss= 0.08928161016937805\n",
            "progress: 53.400000000000006 Training Loss= 0.1348328537223498\n",
            "progress: 53.400000000000006 Test Loss= 0.0892798577778213\n",
            "progress: 53.6 Training Loss= 0.1348288212543084\n",
            "progress: 53.6 Test Loss= 0.08927810563631136\n",
            "progress: 53.800000000000004 Training Loss= 0.13482479169489117\n",
            "progress: 53.800000000000004 Test Loss= 0.08927635375421403\n",
            "progress: 54.0 Training Loss= 0.134820765036604\n",
            "progress: 54.0 Test Loss= 0.08927460214037689\n",
            "progress: 54.2 Training Loss= 0.13481674127197427\n",
            "progress: 54.2 Test Loss= 0.08927285080315067\n",
            "progress: 54.400000000000006 Training Loss= 0.13481272039355077\n",
            "progress: 54.400000000000006 Test Loss= 0.08927109975040977\n",
            "progress: 54.6 Training Loss= 0.1348087023939034\n",
            "progress: 54.6 Test Loss= 0.08926934898957184\n",
            "progress: 54.800000000000004 Training Loss= 0.13480468726562347\n",
            "progress: 54.800000000000004 Test Loss= 0.08926759852761652\n",
            "progress: 55.00000000000001 Training Loss= 0.13480067500132317\n",
            "progress: 55.00000000000001 Test Loss= 0.08926584837110373\n",
            "progress: 55.2 Training Loss= 0.13479666559363604\n",
            "progress: 55.2 Test Loss= 0.08926409852619092\n",
            "progress: 55.400000000000006 Training Loss= 0.13479265903521642\n",
            "progress: 55.400000000000006 Test Loss= 0.08926234899864977\n",
            "progress: 55.60000000000001 Training Loss= 0.13478865531873957\n",
            "progress: 55.60000000000001 Test Loss= 0.08926059979388211\n",
            "progress: 55.800000000000004 Training Loss= 0.13478465443690163\n",
            "progress: 55.800000000000004 Test Loss= 0.08925885091693539\n",
            "progress: 56.00000000000001 Training Loss= 0.1347806563824196\n",
            "progress: 56.00000000000001 Test Loss= 0.08925710237251741\n",
            "progress: 56.2 Training Loss= 0.134776661148031\n",
            "progress: 56.2 Test Loss= 0.0892553541650104\n",
            "progress: 56.39999999999999 Training Loss= 0.13477266872649432\n",
            "progress: 56.39999999999999 Test Loss= 0.08925360629848464\n",
            "progress: 56.599999999999994 Training Loss= 0.1347686791105883\n",
            "progress: 56.599999999999994 Test Loss= 0.08925185877671153\n",
            "progress: 56.8 Training Loss= 0.13476469229311236\n",
            "progress: 56.8 Test Loss= 0.0892501116031759\n",
            "progress: 56.99999999999999 Training Loss= 0.1347607082668864\n",
            "progress: 56.99999999999999 Test Loss= 0.08924836478108845\n",
            "progress: 57.199999999999996 Training Loss= 0.13475672702475067\n",
            "progress: 57.199999999999996 Test Loss= 0.08924661831339684\n",
            "progress: 57.4 Training Loss= 0.13475274855956568\n",
            "progress: 57.4 Test Loss= 0.08924487220279687\n",
            "progress: 57.599999999999994 Training Loss= 0.13474877286421244\n",
            "progress: 57.599999999999994 Test Loss= 0.08924312645174332\n",
            "progress: 57.8 Training Loss= 0.13474479993159186\n",
            "progress: 57.8 Test Loss= 0.08924138106245981\n",
            "progress: 57.99999999999999 Training Loss= 0.1347408297546252\n",
            "progress: 57.99999999999999 Test Loss= 0.08923963603694886\n",
            "progress: 58.199999999999996 Training Loss= 0.1347368623262538\n",
            "progress: 58.199999999999996 Test Loss= 0.08923789137700107\n",
            "progress: 58.4 Training Loss= 0.1347328976394389\n",
            "progress: 58.4 Test Loss= 0.08923614708420431\n",
            "progress: 58.599999999999994 Training Loss= 0.13472893568716188\n",
            "progress: 58.599999999999994 Test Loss= 0.0892344031599522\n",
            "progress: 58.8 Training Loss= 0.13472497646242385\n",
            "progress: 58.8 Test Loss= 0.08923265960545251\n",
            "progress: 59.0 Training Loss= 0.1347210199582459\n",
            "progress: 59.0 Test Loss= 0.08923091642173515\n",
            "progress: 59.199999999999996 Training Loss= 0.13471706616766885\n",
            "progress: 59.199999999999996 Test Loss= 0.08922917360965976\n",
            "progress: 59.4 Training Loss= 0.13471311508375322\n",
            "progress: 59.4 Test Loss= 0.08922743116992297\n",
            "progress: 59.599999999999994 Training Loss= 0.13470916669957933\n",
            "progress: 59.599999999999994 Test Loss= 0.08922568910306568\n",
            "progress: 59.8 Training Loss= 0.13470522100824697\n",
            "progress: 59.8 Test Loss= 0.08922394740947953\n",
            "progress: 60.0 Training Loss= 0.13470127800287554\n",
            "progress: 60.0 Test Loss= 0.08922220608941359\n",
            "progress: 60.199999999999996 Training Loss= 0.13469733767660397\n",
            "progress: 60.199999999999996 Test Loss= 0.08922046514298058\n",
            "progress: 60.4 Training Loss= 0.1346934000225906\n",
            "progress: 60.4 Test Loss= 0.08921872457016264\n",
            "progress: 60.6 Training Loss= 0.13468946503401313\n",
            "progress: 60.6 Test Loss= 0.08921698437081733\n",
            "progress: 60.8 Training Loss= 0.13468553270406877\n",
            "progress: 60.8 Test Loss= 0.08921524454468296\n",
            "progress: 61.0 Training Loss= 0.13468160302597368\n",
            "progress: 61.0 Test Loss= 0.08921350509138391\n",
            "progress: 61.199999999999996 Training Loss= 0.1346776759929635\n",
            "progress: 61.199999999999996 Test Loss= 0.08921176601043579\n",
            "progress: 61.4 Training Loss= 0.13467375159829303\n",
            "progress: 61.4 Test Loss= 0.08921002730125002\n",
            "progress: 61.6 Training Loss= 0.13466982983523604\n",
            "progress: 61.6 Test Loss= 0.08920828896313893\n",
            "progress: 61.8 Training Loss= 0.1346659106970854\n",
            "progress: 61.8 Test Loss= 0.08920655099531982\n",
            "progress: 62.0 Training Loss= 0.13466199417715302\n",
            "progress: 62.0 Test Loss= 0.08920481339691956\n",
            "progress: 62.2 Training Loss= 0.13465808026876977\n",
            "progress: 62.2 Test Loss= 0.08920307616697848\n",
            "progress: 62.4 Training Loss= 0.13465416896528534\n",
            "progress: 62.4 Test Loss= 0.08920133930445447\n",
            "progress: 62.6 Training Loss= 0.1346502602600683\n",
            "progress: 62.6 Test Loss= 0.08919960280822686\n",
            "progress: 62.8 Training Loss= 0.13464635414650594\n",
            "progress: 62.8 Test Loss= 0.08919786667709977\n",
            "progress: 63.0 Training Loss= 0.1346424506180044\n",
            "progress: 63.0 Test Loss= 0.08919613090980585\n",
            "progress: 63.2 Training Loss= 0.13463854966798838\n",
            "progress: 63.2 Test Loss= 0.08919439550500963\n",
            "progress: 63.4 Training Loss= 0.1346346512899013\n",
            "progress: 63.4 Test Loss= 0.08919266046131052\n",
            "progress: 63.6 Training Loss= 0.13463075547720493\n",
            "progress: 63.6 Test Loss= 0.08919092577724617\n",
            "progress: 63.800000000000004 Training Loss= 0.13462686222337988\n",
            "progress: 63.800000000000004 Test Loss= 0.08918919145129527\n",
            "progress: 64.0 Training Loss= 0.13462297152192496\n",
            "progress: 64.0 Test Loss= 0.08918745748188035\n",
            "progress: 64.2 Training Loss= 0.13461908336635758\n",
            "progress: 64.2 Test Loss= 0.08918572386737078\n",
            "progress: 64.4 Training Loss= 0.1346151977502133\n",
            "progress: 64.4 Test Loss= 0.08918399060608499\n",
            "progress: 64.60000000000001 Training Loss= 0.13461131466704615\n",
            "progress: 64.60000000000001 Test Loss= 0.08918225769629326\n",
            "progress: 64.8 Training Loss= 0.13460743411042833\n",
            "progress: 64.8 Test Loss= 0.08918052513622009\n",
            "progress: 65.0 Training Loss= 0.1346035560739503\n",
            "progress: 65.0 Test Loss= 0.08917879292404633\n",
            "progress: 65.2 Training Loss= 0.13459968055122057\n",
            "progress: 65.2 Test Loss= 0.08917706105791175\n",
            "progress: 65.4 Training Loss= 0.13459580753586586\n",
            "progress: 65.4 Test Loss= 0.08917532953591675\n",
            "progress: 65.60000000000001 Training Loss= 0.13459193702153077\n",
            "progress: 65.60000000000001 Test Loss= 0.08917359835612462\n",
            "progress: 65.8 Training Loss= 0.13458806900187817\n",
            "progress: 65.8 Test Loss= 0.08917186751656356\n",
            "progress: 66.0 Training Loss= 0.13458420347058858\n",
            "progress: 66.0 Test Loss= 0.08917013701522844\n",
            "progress: 66.2 Training Loss= 0.13458034042136058\n",
            "progress: 66.2 Test Loss= 0.08916840685008254\n",
            "progress: 66.4 Training Loss= 0.1345764798479105\n",
            "progress: 66.4 Test Loss= 0.08916667701905943\n",
            "progress: 66.60000000000001 Training Loss= 0.1345726217439725\n",
            "progress: 66.60000000000001 Test Loss= 0.08916494752006458\n",
            "progress: 66.8 Training Loss= 0.13456876610329854\n",
            "progress: 66.8 Test Loss= 0.08916321835097685\n",
            "progress: 67.0 Training Loss= 0.1345649129196581\n",
            "progress: 67.0 Test Loss= 0.08916148950965011\n",
            "progress: 67.2 Training Loss= 0.13456106218683853\n",
            "progress: 67.2 Test Loss= 0.08915976099391462\n",
            "progress: 67.4 Training Loss= 0.13455721389864453\n",
            "progress: 67.4 Test Loss= 0.0891580328015786\n",
            "progress: 67.60000000000001 Training Loss= 0.1345533680488985\n",
            "progress: 67.60000000000001 Test Loss= 0.08915630493042932\n",
            "progress: 67.80000000000001 Training Loss= 0.13454952463144026\n",
            "progress: 67.80000000000001 Test Loss= 0.08915457737823458\n",
            "progress: 68.0 Training Loss= 0.134545683640127\n",
            "progress: 68.0 Test Loss= 0.08915285014274392\n",
            "progress: 68.2 Training Loss= 0.13454184506883354\n",
            "progress: 68.2 Test Loss= 0.08915112322168962\n",
            "progress: 68.4 Training Loss= 0.13453800891145173\n",
            "progress: 68.4 Test Loss= 0.08914939661278817\n",
            "progress: 68.60000000000001 Training Loss= 0.13453417516189092\n",
            "progress: 68.60000000000001 Test Loss= 0.08914767031374095\n",
            "progress: 68.8 Training Loss= 0.1345303438140776\n",
            "progress: 68.8 Test Loss= 0.08914594432223573\n",
            "progress: 69.0 Training Loss= 0.13452651486195547\n",
            "progress: 69.0 Test Loss= 0.0891442186359472\n",
            "progress: 69.19999999999999 Training Loss= 0.1345226882994854\n",
            "progress: 69.19999999999999 Test Loss= 0.08914249325253829\n",
            "progress: 69.39999999999999 Training Loss= 0.13451886412064534\n",
            "progress: 69.39999999999999 Test Loss= 0.08914076816966086\n",
            "progress: 69.6 Training Loss= 0.13451504231943026\n",
            "progress: 69.6 Test Loss= 0.08913904338495678\n",
            "progress: 69.8 Training Loss= 0.13451122288985218\n",
            "progress: 69.8 Test Loss= 0.0891373188960585\n",
            "progress: 70.0 Training Loss= 0.13450740582594\n",
            "progress: 70.0 Test Loss= 0.08913559470059014\n",
            "progress: 70.19999999999999 Training Loss= 0.13450359112173957\n",
            "progress: 70.19999999999999 Test Loss= 0.08913387079616811\n",
            "progress: 70.39999999999999 Training Loss= 0.13449977877131356\n",
            "progress: 70.39999999999999 Test Loss= 0.08913214718040187\n",
            "progress: 70.6 Training Loss= 0.13449596876874145\n",
            "progress: 70.6 Test Loss= 0.08913042385089466\n",
            "progress: 70.8 Training Loss= 0.13449216110811948\n",
            "progress: 70.8 Test Loss= 0.0891287008052442\n",
            "progress: 71.0 Training Loss= 0.13448835578356066\n",
            "progress: 71.0 Test Loss= 0.08912697804104325\n",
            "progress: 71.2 Training Loss= 0.13448455278919455\n",
            "progress: 71.2 Test Loss= 0.08912525555588033\n",
            "progress: 71.39999999999999 Training Loss= 0.13448075211916732\n",
            "progress: 71.39999999999999 Test Loss= 0.08912353334734029\n",
            "progress: 71.6 Training Loss= 0.13447695376764182\n",
            "progress: 71.6 Test Loss= 0.08912181141300488\n",
            "progress: 71.8 Training Loss= 0.13447315772879728\n",
            "progress: 71.8 Test Loss= 0.08912008975045328\n",
            "progress: 72.0 Training Loss= 0.13446936399682966\n",
            "progress: 72.0 Test Loss= 0.0891183683572627\n",
            "progress: 72.2 Training Loss= 0.134465572565951\n",
            "progress: 72.2 Test Loss= 0.08911664723100868\n",
            "progress: 72.39999999999999 Training Loss= 0.13446178343038995\n",
            "progress: 72.39999999999999 Test Loss= 0.08911492636926585\n",
            "progress: 72.6 Training Loss= 0.13445799658439136\n",
            "progress: 72.6 Test Loss= 0.08911320576960818\n",
            "progress: 72.8 Training Loss= 0.1344542120222165\n",
            "progress: 72.8 Test Loss= 0.08911148542960957\n",
            "progress: 73.0 Training Loss= 0.13445042973814278\n",
            "progress: 73.0 Test Loss= 0.08910976534684409\n",
            "progress: 73.2 Training Loss= 0.13444664972646383\n",
            "progress: 73.2 Test Loss= 0.08910804551888656\n",
            "progress: 73.4 Training Loss= 0.13444287198148938\n",
            "progress: 73.4 Test Loss= 0.0891063259433128\n",
            "progress: 73.6 Training Loss= 0.1344390964975454\n",
            "progress: 73.6 Test Loss= 0.08910460661770014\n",
            "progress: 73.8 Training Loss= 0.13443532326897378\n",
            "progress: 73.8 Test Loss= 0.08910288753962763\n",
            "progress: 74.0 Training Loss= 0.13443155229013246\n",
            "progress: 74.0 Test Loss= 0.0891011687066765\n",
            "progress: 74.2 Training Loss= 0.1344277835553953\n",
            "progress: 74.2 Test Loss= 0.08909945011643033\n",
            "progress: 74.4 Training Loss= 0.13442401705915225\n",
            "progress: 74.4 Test Loss= 0.08909773176647554\n",
            "progress: 74.6 Training Loss= 0.13442025279580885\n",
            "progress: 74.6 Test Loss= 0.08909601365440153\n",
            "progress: 74.8 Training Loss= 0.1344164907597868\n",
            "progress: 74.8 Test Loss= 0.08909429577780108\n",
            "progress: 75.0 Training Loss= 0.1344127309455233\n",
            "progress: 75.0 Test Loss= 0.08909257813427049\n",
            "progress: 75.2 Training Loss= 0.13440897334747137\n",
            "progress: 75.2 Test Loss= 0.08909086072141002\n",
            "progress: 75.4 Training Loss= 0.13440521796009983\n",
            "progress: 75.4 Test Loss= 0.0890891435368239\n",
            "progress: 75.6 Training Loss= 0.1344014647778931\n",
            "progress: 75.6 Test Loss= 0.08908742657812084\n",
            "progress: 75.8 Training Loss= 0.1343977137953511\n",
            "progress: 75.8 Test Loss= 0.08908570984291392\n",
            "progress: 76.0 Training Loss= 0.13439396500698936\n",
            "progress: 76.0 Test Loss= 0.0890839933288211\n",
            "progress: 76.2 Training Loss= 0.13439021840733906\n",
            "progress: 76.2 Test Loss= 0.0890822770334653\n",
            "progress: 76.4 Training Loss= 0.13438647399094672\n",
            "progress: 76.4 Test Loss= 0.08908056095447457\n",
            "progress: 76.6 Training Loss= 0.13438273175237425\n",
            "progress: 76.6 Test Loss= 0.08907884508948233\n",
            "progress: 76.8 Training Loss= 0.13437899168619905\n",
            "progress: 76.8 Test Loss= 0.08907712943612746\n",
            "progress: 77.0 Training Loss= 0.13437525378701387\n",
            "progress: 77.0 Test Loss= 0.08907541399205463\n",
            "progress: 77.2 Training Loss= 0.13437151804942654\n",
            "progress: 77.2 Test Loss= 0.0890736987549143\n",
            "progress: 77.4 Training Loss= 0.13436778446806052\n",
            "progress: 77.4 Test Loss= 0.08907198372236291\n",
            "progress: 77.60000000000001 Training Loss= 0.13436405303755417\n",
            "progress: 77.60000000000001 Test Loss= 0.0890702688920631\n",
            "progress: 77.8 Training Loss= 0.1343603237525611\n",
            "progress: 77.8 Test Loss= 0.08906855426168367\n",
            "progress: 78.0 Training Loss= 0.13435659660775007\n",
            "progress: 78.0 Test Loss= 0.08906683982889999\n",
            "progress: 78.2 Training Loss= 0.13435287159780496\n",
            "progress: 78.2 Test Loss= 0.08906512559139387\n",
            "progress: 78.4 Training Loss= 0.1343491487174246\n",
            "progress: 78.4 Test Loss= 0.08906341154685381\n",
            "progress: 78.60000000000001 Training Loss= 0.13434542796132284\n",
            "progress: 78.60000000000001 Test Loss= 0.089061697692975\n",
            "progress: 78.8 Training Loss= 0.13434170932422854\n",
            "progress: 78.8 Test Loss= 0.08905998402745964\n",
            "progress: 79.0 Training Loss= 0.1343379928008854\n",
            "progress: 79.0 Test Loss= 0.08905827054801678\n",
            "progress: 79.2 Training Loss= 0.134334278386052\n",
            "progress: 79.2 Test Loss= 0.08905655725236254\n",
            "progress: 79.4 Training Loss= 0.1343305660745018\n",
            "progress: 79.4 Test Loss= 0.08905484413822028\n",
            "progress: 79.60000000000001 Training Loss= 0.134326855861023\n",
            "progress: 79.60000000000001 Test Loss= 0.08905313120332049\n",
            "progress: 79.80000000000001 Training Loss= 0.13432314774041854\n",
            "progress: 79.80000000000001 Test Loss= 0.0890514184454011\n",
            "progress: 80.0 Training Loss= 0.13431944170750607\n",
            "progress: 80.0 Test Loss= 0.08904970586220738\n",
            "progress: 80.2 Training Loss= 0.1343157377571179\n",
            "progress: 80.2 Test Loss= 0.08904799345149203\n",
            "progress: 80.4 Training Loss= 0.1343120358841009\n",
            "progress: 80.4 Test Loss= 0.08904628121101534\n",
            "progress: 80.60000000000001 Training Loss= 0.13430833608331666\n",
            "progress: 80.60000000000001 Test Loss= 0.0890445691385452\n",
            "progress: 80.80000000000001 Training Loss= 0.13430463834964113\n",
            "progress: 80.80000000000001 Test Loss= 0.0890428572318572\n",
            "progress: 81.0 Training Loss= 0.13430094267796477\n",
            "progress: 81.0 Test Loss= 0.08904114548873446\n",
            "progress: 81.2 Training Loss= 0.13429724906319276\n",
            "progress: 81.2 Test Loss= 0.0890394339069681\n",
            "progress: 81.39999999999999 Training Loss= 0.1342935575002443\n",
            "progress: 81.39999999999999 Test Loss= 0.08903772248435692\n",
            "progress: 81.6 Training Loss= 0.1342898679840532\n",
            "progress: 81.6 Test Loss= 0.0890360112187077\n",
            "progress: 81.8 Training Loss= 0.13428618050956753\n",
            "progress: 81.8 Test Loss= 0.08903430010783506\n",
            "progress: 82.0 Training Loss= 0.1342824950717497\n",
            "progress: 82.0 Test Loss= 0.0890325891495615\n",
            "progress: 82.19999999999999 Training Loss= 0.13427881166557623\n",
            "progress: 82.19999999999999 Test Loss= 0.08903087834171759\n",
            "progress: 82.39999999999999 Training Loss= 0.134275130286038\n",
            "progress: 82.39999999999999 Test Loss= 0.08902916768214197\n",
            "progress: 82.6 Training Loss= 0.13427145092813997\n",
            "progress: 82.6 Test Loss= 0.08902745716868125\n",
            "progress: 82.8 Training Loss= 0.13426777358690128\n",
            "progress: 82.8 Test Loss= 0.08902574679919016\n",
            "progress: 83.0 Training Loss= 0.13426409825735508\n",
            "progress: 83.0 Test Loss= 0.08902403657153153\n",
            "progress: 83.2 Training Loss= 0.13426042493454865\n",
            "progress: 83.2 Test Loss= 0.08902232648357629\n",
            "progress: 83.39999999999999 Training Loss= 0.13425675361354322\n",
            "progress: 83.39999999999999 Test Loss= 0.0890206165332036\n",
            "progress: 83.6 Training Loss= 0.13425308428941402\n",
            "progress: 83.6 Test Loss= 0.08901890671830082\n",
            "progress: 83.8 Training Loss= 0.13424941695725015\n",
            "progress: 83.8 Test Loss= 0.08901719703676333\n",
            "progress: 84.0 Training Loss= 0.13424575161215463\n",
            "progress: 84.0 Test Loss= 0.08901548748649495\n",
            "progress: 84.2 Training Loss= 0.1342420882492444\n",
            "progress: 84.2 Test Loss= 0.08901377806540754\n",
            "progress: 84.39999999999999 Training Loss= 0.1342384268636501\n",
            "progress: 84.39999999999999 Test Loss= 0.08901206877142144\n",
            "progress: 84.6 Training Loss= 0.1342347674505161\n",
            "progress: 84.6 Test Loss= 0.08901035960246505\n",
            "progress: 84.8 Training Loss= 0.13423111000500068\n",
            "progress: 84.8 Test Loss= 0.08900865055647503\n",
            "progress: 85.0 Training Loss= 0.1342274545222757\n",
            "progress: 85.0 Test Loss= 0.08900694163139644\n",
            "progress: 85.2 Training Loss= 0.13422380099752662\n",
            "progress: 85.2 Test Loss= 0.08900523282518252\n",
            "progress: 85.39999999999999 Training Loss= 0.1342201494259526\n",
            "progress: 85.39999999999999 Test Loss= 0.08900352413579483\n",
            "progress: 85.6 Training Loss= 0.13421649980276631\n",
            "progress: 85.6 Test Loss= 0.08900181556120326\n",
            "progress: 85.8 Training Loss= 0.1342128521231941\n",
            "progress: 85.8 Test Loss= 0.08900010709938591\n",
            "progress: 86.0 Training Loss= 0.13420920638247558\n",
            "progress: 86.0 Test Loss= 0.08899839874832922\n",
            "progress: 86.2 Training Loss= 0.13420556257586397\n",
            "progress: 86.2 Test Loss= 0.08899669050602797\n",
            "progress: 86.4 Training Loss= 0.13420192069862594\n",
            "progress: 86.4 Test Loss= 0.08899498237048516\n",
            "progress: 86.6 Training Loss= 0.13419828074604145\n",
            "progress: 86.6 Test Loss= 0.08899327433971216\n",
            "progress: 86.8 Training Loss= 0.13419464271340384\n",
            "progress: 86.8 Test Loss= 0.08899156641172846\n",
            "progress: 87.0 Training Loss= 0.13419100659601976\n",
            "progress: 87.0 Test Loss= 0.0889898585845621\n",
            "progress: 87.2 Training Loss= 0.13418737238920914\n",
            "progress: 87.2 Test Loss= 0.08898815085624914\n",
            "progress: 87.4 Training Loss= 0.13418374008830514\n",
            "progress: 87.4 Test Loss= 0.08898644322483418\n",
            "progress: 87.6 Training Loss= 0.13418010968865404\n",
            "progress: 87.6 Test Loss= 0.08898473568836977\n",
            "progress: 87.8 Training Loss= 0.13417648118561537\n",
            "progress: 87.8 Test Loss= 0.08898302824491704\n",
            "progress: 88.0 Training Loss= 0.13417285457456174\n",
            "progress: 88.0 Test Loss= 0.08898132089254515\n",
            "progress: 88.2 Training Loss= 0.1341692298508789\n",
            "progress: 88.2 Test Loss= 0.08897961362933163\n",
            "progress: 88.4 Training Loss= 0.13416560700996552\n",
            "progress: 88.4 Test Loss= 0.08897790645336212\n",
            "progress: 88.6 Training Loss= 0.13416198604723334\n",
            "progress: 88.6 Test Loss= 0.08897619936273064\n",
            "progress: 88.8 Training Loss= 0.13415836695810712\n",
            "progress: 88.8 Test Loss= 0.08897449235553924\n",
            "progress: 89.0 Training Loss= 0.13415474973802446\n",
            "progress: 89.0 Test Loss= 0.08897278542989842\n",
            "progress: 89.2 Training Loss= 0.134151134382436\n",
            "progress: 89.2 Test Loss= 0.08897107858392657\n",
            "progress: 89.4 Training Loss= 0.134147520886805\n",
            "progress: 89.4 Test Loss= 0.08896937181575049\n",
            "progress: 89.60000000000001 Training Loss= 0.13414390924660782\n",
            "progress: 89.60000000000001 Test Loss= 0.08896766512350504\n",
            "progress: 89.8 Training Loss= 0.13414029945733347\n",
            "progress: 89.8 Test Loss= 0.08896595850533319\n",
            "progress: 90.0 Training Loss= 0.1341366915144836\n",
            "progress: 90.0 Test Loss= 0.08896425195938619\n",
            "progress: 90.2 Training Loss= 0.13413308541357272\n",
            "progress: 90.2 Test Loss= 0.08896254548382326\n",
            "progress: 90.4 Training Loss= 0.13412948115012807\n",
            "progress: 90.4 Test Loss= 0.08896083907681177\n",
            "progress: 90.60000000000001 Training Loss= 0.13412587871968934\n",
            "progress: 90.60000000000001 Test Loss= 0.08895913273652716\n",
            "progress: 90.8 Training Loss= 0.13412227811780905\n",
            "progress: 90.8 Test Loss= 0.08895742646115298\n",
            "progress: 91.0 Training Loss= 0.13411867934005206\n",
            "progress: 91.0 Test Loss= 0.08895572024888086\n",
            "progress: 91.2 Training Loss= 0.134115082381996\n",
            "progress: 91.2 Test Loss= 0.0889540140979104\n",
            "progress: 91.4 Training Loss= 0.13411148723923072\n",
            "progress: 91.4 Test Loss= 0.0889523080064492\n",
            "progress: 91.60000000000001 Training Loss= 0.13410789390735886\n",
            "progress: 91.60000000000001 Test Loss= 0.08895060197271293\n",
            "progress: 91.8 Training Loss= 0.1341043023819952\n",
            "progress: 91.8 Test Loss= 0.08894889599492518\n",
            "progress: 92.0 Training Loss= 0.13410071265876708\n",
            "progress: 92.0 Test Loss= 0.08894719007131759\n",
            "progress: 92.2 Training Loss= 0.13409712473331412\n",
            "progress: 92.2 Test Loss= 0.08894548420012963\n",
            "progress: 92.4 Training Loss= 0.1340935386012883\n",
            "progress: 92.4 Test Loss= 0.08894377837960879\n",
            "progress: 92.60000000000001 Training Loss= 0.1340899542583539\n",
            "progress: 92.60000000000001 Test Loss= 0.08894207260801046\n",
            "progress: 92.80000000000001 Training Loss= 0.13408637170018745\n",
            "progress: 92.80000000000001 Test Loss= 0.08894036688359784\n",
            "progress: 93.0 Training Loss= 0.13408279092247757\n",
            "progress: 93.0 Test Loss= 0.08893866120464206\n",
            "progress: 93.2 Training Loss= 0.13407921192092528\n",
            "progress: 93.2 Test Loss= 0.08893695556942209\n",
            "progress: 93.4 Training Loss= 0.13407563469124364\n",
            "progress: 93.4 Test Loss= 0.08893524997622476\n",
            "progress: 93.60000000000001 Training Loss= 0.13407205922915774\n",
            "progress: 93.60000000000001 Test Loss= 0.08893354442334463\n",
            "progress: 93.8 Training Loss= 0.1340684855304049\n",
            "progress: 93.8 Test Loss= 0.08893183890908408\n",
            "progress: 94.0 Training Loss= 0.1340649135907344\n",
            "progress: 94.0 Test Loss= 0.08893013343175327\n",
            "progress: 94.19999999999999 Training Loss= 0.13406134340590756\n",
            "progress: 94.19999999999999 Test Loss= 0.08892842798967011\n",
            "progress: 94.39999999999999 Training Loss= 0.13405777497169757\n",
            "progress: 94.39999999999999 Test Loss= 0.08892672258116022\n",
            "progress: 94.6 Training Loss= 0.1340542082838898\n",
            "progress: 94.6 Test Loss= 0.08892501720455688\n",
            "progress: 94.8 Training Loss= 0.13405064333828126\n",
            "progress: 94.8 Test Loss= 0.08892331185820115\n",
            "progress: 95.0 Training Loss= 0.1340470801306811\n",
            "progress: 95.0 Test Loss= 0.08892160654044165\n",
            "progress: 95.19999999999999 Training Loss= 0.13404351865691005\n",
            "progress: 95.19999999999999 Test Loss= 0.08891990124963467\n",
            "progress: 95.39999999999999 Training Loss= 0.1340399589128008\n",
            "progress: 95.39999999999999 Test Loss= 0.08891819598414416\n",
            "progress: 95.6 Training Loss= 0.13403640089419777\n",
            "progress: 95.6 Test Loss= 0.08891649074234159\n",
            "progress: 95.8 Training Loss= 0.13403284459695716\n",
            "progress: 95.8 Test Loss= 0.08891478552260608\n",
            "progress: 96.0 Training Loss= 0.13402929001694688\n",
            "progress: 96.0 Test Loss= 0.08891308032332422\n",
            "progress: 96.2 Training Loss= 0.13402573715004645\n",
            "progress: 96.2 Test Loss= 0.08891137514289017\n",
            "progress: 96.39999999999999 Training Loss= 0.13402218599214705\n",
            "progress: 96.39999999999999 Test Loss= 0.08890966997970563\n",
            "progress: 96.6 Training Loss= 0.1340186365391515\n",
            "progress: 96.6 Test Loss= 0.0889079648321797\n",
            "progress: 96.8 Training Loss= 0.13401508878697413\n",
            "progress: 96.8 Test Loss= 0.08890625969872898\n",
            "progress: 97.0 Training Loss= 0.13401154273154098\n",
            "progress: 97.0 Test Loss= 0.08890455457777754\n",
            "progress: 97.2 Training Loss= 0.13400799836878932\n",
            "progress: 97.2 Test Loss= 0.08890284946775683\n",
            "progress: 97.39999999999999 Training Loss= 0.13400445569466818\n",
            "progress: 97.39999999999999 Test Loss= 0.08890114436710571\n",
            "progress: 97.6 Training Loss= 0.13400091470513786\n",
            "progress: 97.6 Test Loss= 0.08889943927427038\n",
            "progress: 97.8 Training Loss= 0.13399737539617007\n",
            "progress: 97.8 Test Loss= 0.08889773418770437\n",
            "progress: 98.0 Training Loss= 0.133993837763748\n",
            "progress: 98.0 Test Loss= 0.08889602910586866\n",
            "progress: 98.2 Training Loss= 0.13399030180386615\n",
            "progress: 98.2 Test Loss= 0.08889432402723138\n",
            "progress: 98.4 Training Loss= 0.13398676751253027\n",
            "progress: 98.4 Test Loss= 0.08889261895026804\n",
            "progress: 98.6 Training Loss= 0.1339832348857575\n",
            "progress: 98.6 Test Loss= 0.08889091387346139\n",
            "progress: 98.8 Training Loss= 0.1339797039195761\n",
            "progress: 98.8 Test Loss= 0.0888892087953013\n",
            "progress: 99.0 Training Loss= 0.1339761746100257\n",
            "progress: 99.0 Test Loss= 0.08888750371428507\n",
            "progress: 99.2 Training Loss= 0.1339726469531569\n",
            "progress: 99.2 Test Loss= 0.08888579862891698\n",
            "progress: 99.4 Training Loss= 0.13396912094503172\n",
            "progress: 99.4 Test Loss= 0.08888409353770858\n",
            "progress: 99.6 Training Loss= 0.13396559658172308\n",
            "progress: 99.6 Test Loss= 0.08888238843917858\n",
            "progress: 99.8 Training Loss= 0.13396207385931513\n",
            "progress: 99.8 Test Loss= 0.0888806833318527\n",
            "progress: 100.0 Training Loss= 0.13395855277390303\n",
            "progress: 100.0 Test Loss= 0.0888789782142639\n",
            "[ 0.24767748  0.99165538  0.6322877  -0.77864816 -2.53409425 -2.6897393\n",
            "  1.56586711  1.27757868  0.89303065]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Training Data Result:"
      ],
      "metadata": {
        "id": "TPtGIcbw5Uoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T_Pred = calcPoly(X, vPolyParams)\n",
        "T_Pred_test = calcPoly(X_test, vPolyParams)\n",
        "plt.scatter(X, T) \n",
        "plt.scatter(X, T_Pred)\n",
        "plt.scatter(X_test, T_Pred_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "r3R0wIOqqTHk",
        "outputId": "4be79fa8-26f4-4fbc-960d-10405181ad7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7ff2b248e610>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAA0fUlEQVR4nO3de3xc5XXo/d+a0diSjK2RbCeSZYNtSk0PYBC4lB6T1ITEThgC5mbgbVpo0nLaJgcBqYtpHCOMe2zqExL1bU4JLZyQJi/4AgiDwrFzuJRAjhNkZJtL4r6OHcBCCgZZMuhiaWae88dcPJe9Z0Yze0ZzWd/PRx9rZrY0e1sza569nrXXI8YYlFJKlT/XZO+AUkqpwtCAr5RSFUIDvlJKVQgN+EopVSE04CulVIWomuwdsDNr1iwzf/78yd4NpZQqKXv27PnAGDPb6rGiDfjz58+nq6trsndDKaVKioi8bfeYpnSUUqpCaMBXSqkKoQFfKaUqhAZ8pZSqEBrwlVKqQhRtlY5S5aKju4fNOw/w3sAIc7w1rF6xiJUtzWkfU8ppGvCVclhsEK+r8TA05mc8EOpK2zMwwu1b9tL1dj9LTmvgrideZ2Q8EH3srideB9Cgr/JCirU98pIlS4zW4atS09HdExfEU5k2xc3QWPJ2zd4aXlnzmXzsnqoAIrLHGLPE6rGcc/giMk9EXhCRt0TkTRFptdhGROQfReSgiOwXkfNzfV6litHmnQcyCvaAZbAHeG9gxMldUirKiZSOH/i6MeY1EZkO7BGRnxhj3orZ5gvAGeGvPwD+OfyvUmXFiWA9x1vjwJ4olSznEb4xptcY81r4+4+AXwKJCcgrgR+YkN2AV0Sacn1upYpNrsG6xuNm9YpFDu2NUvEcLcsUkflAC/DzhIeagXdjbh8h+UMBEblFRLpEpOvo0aNO7ppSBbF6xSJqPO64+1K9ybw1Hpq9NQih3P3Gq8/RCVuVN45V6YjIKcDjwG3GmOPZ/A5jzIPAgxCatHVq35QqlEiwTiy17Hq7nx/tfofYF3WNx03bFWdpgFcF40iVjoh4gGeAncaY+y0e/x7wojHm0fDtA8AyY0yv3e/UKh1VbhLLNUVgYHhc6++Vo1JV6eQc8EVEgEeAfmPMbTbb+ICvAZcRmqz9R2PMhal+rwZ8Va6sSjc9LuGU6ir9AFA5SxXwnUjpLAX+BHhdRPaG7/s74FQAY8wDwI8JBfuDwDDwZw48r1Ilyap0czxoODY8DugFWCp/cg74xpiXAUmzjQG+mutzKVUOMindHBkPsHnnAQ34ylHaWkGpAojN37tECGSQStULsJTTNOArlWeJOftMgj3oBVjKeRrwlXJAqq6Xdu0W3CIEjUlqsAahHOklZ1quQ61U1rQfvlI5iozgewZGMJycdO3o7gHsUzMBYzi8ycfeu5dz/e/Pi5sIM8Dje3qiv0MpJ2jAVypHViP4yKQr2KdmBKIB/YVfHSUx0RP7O5RyggZ8pXLUYzOC7xkYYcGaToZO+C0fNxAN6HZnATpxq5ykAV+pHHR096SsSTbAwMi47eORDwu7swCduFVO0oCvVA427zyQlIqZCLeEPi6smq5p50zlNK3SUSoHuaZcIiWadk3X9MIr5SQN+ErlYI63xjaHn4lmb01SSee3rz9PA73KCw34SuVg9YpFlo3QEOLq6q3uq/G4ueTM2ZYLmXe93c8Lvzqqo33lKA34SuXALhWT6X12JZ0/3P1O9LY2U1NOcaQffj5oe2RVCRas6cx40rfZW8Mraz6T1/1RpS/f7ZGLSqpL3JUqNhOZA9CafJWrsirLTHeJu1LFxqoc047W5KtcldUIP9Ul7jrKd56eTeUudg4g1Uhfa/KVE8oq4Ovl6YWT2PK3nCcWU32wOfGht7KlmZUtzZZLHwLU13q4+4u62LnKXVkFfLt8qJ4KO69SzqZSfbABjn7o6cVXKt/KKuBb1UTrqbC9XEanqRqGlZN0nTCd/tCLjPaVyoeyCvg6QspcrikZt80yfW6RssrtZ5Mm1BSiKlZlFfBBR0iZyjUlY7dMX8CYssrtp0oTDo/5OTac3AnTW+spxK4pNWFlVZapMpfpyLWju4elm55nwZpOlm56Plri2mwzL+IWSZkCKTWpuljaXbNYpNcyKqUBv1Jl0n891XUNdoHQbuRfqmmOlS3NbLz6HJq9NQihD7qNV5/DypZmBm363Nvdr9RkK7uUjsqMXdOv4TE/C9Z0RlMWdqP1yCX+Vr1hyq1Syi5NqFVhqtToCL9CJY5cvTUeEDg2PB4dzVvlp+HkaH1lSzOrVyxijreG9wZG2LzzAJecObtiFvLQRUtUqdERfgWLHbku3fR8yqX4YrnClTiQXIf++J4errmguSJa+2pVmCo12i1TARPr2gihkWy1x2V5FlAuXR3LqbxUVY5U3TI1paMA+7yzt8YTXXc11sh4IG3Kp5RpIz5VjjTgK8A+H912xVkEJ3gWWA6TlumusFWqFDkS8EXkYRF5X0TesHl8mYgMisje8Nc6J55XOSdV+WGq0X+5TlranaWUW+sIVVmcmrT9PvBPwA9SbPNTY8zlDj2fLc27Zs+u/NCuR1HbFWcBqSctS/XvYVdyKYSOqRSOQalEjgR8Y8xLIjLfid+Vi0pq2VtI6apR7P5vS/nvsXrFIm7fsjdpIttA2XUEVZWjkGWZfygi+4D3gL8xxryZuIGI3ALcAnDqqadO+AkqpWVvtnIZbWfTo6iU/x4rW5q5bctey8fKYVJaVaZCBfzXgNOMMR+LyGVAB3BG4kbGmAeBByFUljnRJ9EFUOxlMtp2Ov1S6n+PZr2SVpWZglTpGGOOG2M+Dn//Y8AjIrOcfp5M+sOAfUOwcpau6iQfZYiZ/j2KlV5Jq8pNQQK+iDSKhIq5ReTC8PN+6PTzZPIGrdT66nSj7XyUIVr9PWL79RTDh22qD/9UlUtKlSJHUjoi8iiwDJglIkeAuwEPgDHmAeBa4K9ExA+MADeYPFzim8ml7qWcV85FukZf+Ui/JP496mo8DMX0kJ/sSVyrNNdtW/Zyz9NvRteQ1fUVVDkp+9YKiXlpuzpqAQ5v8uX8fMXKaoHsGo87OmJduul5y/8bJ9skFOI5nNgfCL0eDKF9K5VSUqWgglsrWKVvkpsEhJRKXjlb6dIThchXp7qYaTLSO6nOXiLDoEpJ+anKUNbdMq3SN4aTo7eISpmIS5WeKETnx1RnWJOR3km1P7GKPeVXqhe3qcIr65ROqg6QzeEe7voGKZyO7h5Wb9vHeND+NZdNeqfzxW/SfuhJ+lzQGITWhVfhW3Yv7N9K50/X0z41QG+VGxcQhOi/dYEgIjDoctHoD3Br/wC+oWH6zSn871Om8P/OrGHQHX8SLIARoWlaE63nt+JbOLlpwHSpOlV5UqV0ynqEbzeCi+RlI6OiSCWKvkGcYzXqBLDNqYVF0yzhYL2xOhgXdCNnZ03+AK3HBgChbVY9o+Ftet3QdvhJ+OAgvPMz2upPYdQVepkHw78j8u9g1ckUVq+nintmNxCqJQuyaVYtfldyxjPyUdU71Evbz9rgtR/ie+NZNjTUsWXG9Lhta4xhqjGhD5QpXlovusvxD4hKLUJQ2SnrEb7d6OeaC5p5fE+PjoryxOr/HcAlEDQw9ZMdeOp3h2N/7OtPwtsJQRMOyxatmSOqg0GqjWHA7U56rMnvBxMK5BPRNO4HMv+5pnE/nx4eDgX7FPsa2d8rP/qYl2pr6aty0yhVtH7q73P6ELA7iy33IgRlr2JH+HZ5aR0V5Vfs/+/13u/QPesIfVVuZgSCHHe5MEJMcEwOkkFM2uAJMOpyMWozYOmz+BDIRF/VxH6ur8rNtgyCPYT2N/aDoZcAbf/+t/D4X+AbGgptVNMAX7gPFq/K6Pl1XV01EWUd8MF6ovJ27ZGSF52HOml/rZ3jjb38zqwAfzQ8xI7pJ1MqgxMMprloDASyGuE3+kMfVJn+XKM/ND+QsYQPhlGXi/b6umjA73SN0v6Lu+l7bT2NAUPr6VeH5iNsXHLmbH60+52KLEJQE1f2Ad9KpYyK8lm90Xmok40/XcugCV1EVWMMfhHGRUDgtx43WzMc+eaiLhDkhEsYjcm3VwcNrXXnxeTwM6s+rg4Gw/MCsHZWg2UO32r7v5s9MzovkI3IWUXntFraZjVE97e3Smg7/ATsewzfJX+fNOrv6O7h8T09SUmxay7Qi8WUtYoM+Hb93ctpVORYa+L9W+G59fxF9Qi7a2I+ECOBPPzviFVgz3Owrw4Guav/GADtDfX0uV1JVTr8dD3tU/0ZV+lcNjTMoExnw5QxNp7iSV2lw0x8b/+Y7qlTMsrhY6xTVZGzivZ6b9KH06jLRfu0KnxP3xq6Iybo25Udv/Cro2n+51SlqsiAX4ia88mW1TzFM3fAnu+DCYC42TB/EdvMRwTrAWryHsAtJeToE6t0fMMn4IKb8V1+f/LPLl6Fb/Eq7KYuYyc8jwNfC38JcLjVZ/tzcarvYG3XwwCpq3T8AT49PMxT009JOBs5eVZhN3/QV+WG8RF4bn1cwH8/+DOmnb4T8Qxgxr2cOLoC//EWTU0qWxUZ8CG7/u6lJKPeOFZ16qfNockf4LTxMXabj3IP8jaj2rjHw0RcGAwucRE0wbzXujuS2rv8frj8ftYCa+222b8Vnr4NxodoOTFGe703VKUT+dAaGgbC8wEWcweRMwAGj0Tv6zzUSXXTE+AKpdRkygD1TY/yN1UP8cuxP4XMPq5UhanYgF/urILZosb/QZ/3bc75/p24gAtHTrC3dkpSnXqvpyo0EZljsE8sQ5wRDDImbkYkFOS9wSBrZpyL79pHc3qebF1y5mx+uPsdy/sdtXhVdGTuC39FUmWEgz1A67GBuBw+xJ8BUDc3en/7a+3RYB8x6nLxUEM1z/Z8F/aflXGlj6ocGvDL1OoVi3j5yf/BbTzGHPmAdTM/wVMzpkaDeBDYXTPVPqhnEexdxjAjGGTQ7abRH6S1vz80eh1xTajUsFDsct0FyYHHfAgAsH8rvmfvhA/6rc8APDVw6bro5n1DfZa/tq/Kzf+q9dDetZ7e7nsLdrakSkNZX3hVcSKjxsEjUFNPYPQj3OEqmnPnzyOYjxy8iRmtDw7ju/Qfii6wx4pULqXqoVMUFy3F/i3r5oaCfcz/6/Lty+kd6k36sTp/IKlqKcoYag2sW3BVylJPVdoq9sKrivLMHdD1MNErV0f6iZ0CzKpsMDH/HjM4mOKqYv3FuV0lWmh2VwAnKory3MQzgASt57fS9tKdjMb8faqDoaoj2zJUEYYFvvmbJ6HdutRTlTcN+KVk/1Z49k4Y6Y+/3zMNxodS/mikFDGJzaRqdTDIeZ6Z/CI4SNAEcYmL6xZdx9qLbKcmi55V5VKiUinP9S30wTu7af/1dvrcJ9M/d82emfZnx0VsSz1VedOAH5Z4kdIlZ87mhV8dLZqyzc7tN9I+sJe+T06j0V8dV92RLtgDXHf8o+RacWO46ESAt92EqnTERRBTtvneTMoVS6mfkm/ZvfgazokbBLQ3NNBblT51Z1fqqcqb5vDJ7FS/UM3VIu0J+ob6aJzWSOv5rfDObtoOP5FUvdH2Qf/JoJ+BDTMb2DZ9WvTio+tmLWHt5d93/BiKReKH+HDM8opWJmvlLSd1Huqk7WdtjAZGU27XNO5n15H3Qjfq5tnOFajSkyqHrwGf1Evdxcp3QLB6s1a7q6keH2HAlTxqi3vT2qlpgJFjFfdmtvoQ97hCbR/GA8mv+XLqlhoZNFhN6gJ4jOHeox+GBwtC57Qa7pnZwEj4NSbiYtWiVSWdvqtkOmmbRqZXJublCsaYaoz2U+cy6k5orhUYZdTmDD11Z0eBJV8OXRhUgazy9eNBg7fGw7SpVfQMjOAWIWBM2a1b61voi6bjOg91svGVexgMhM4EvcEgaz48Fhfs75rVgIk5ezQYthzYAqBBv8yU9Zq2mcq0KsMl4uzapvu3wtO3wuC7gKFvgn+N6BWYAOIOjeaR0Cn61Q9WbLAH+w/nwZFxXlnzGX6zyce3Vp0bXfls884DZblurW+hj5f/5Be8fvMbvH7+On466MI3NBJ6jWBor/fGBftY2371GLTVwX0LQq9VVfJ0hI91MzUrAWMyb0AWHrl3+vvZOLOewfDpstfAmkgd9HPrQxNnYXaX1te5aznhH2bUFV+CF70Cc4I91CtBqvVqW9bv4tjweNzaxpOxpm7BJZZ6fvts+lJEgGhV10g/PPXVk79DlSwd4RN6g2+8+hyavTUIoVz9ly46FbdFuWKkAVlK+7dCx1/S6f+QtbPqQx0XRUCEAZfwzd88SeeL34zrjQKhS+urg/HFk9Xuau5aejdtC66iKWAQY2gKGNoWXI1vdS+0DcKdh8v6jdjR3cN59+xi/ppO5q/ppGX9rrSj8dUrFlHjSU55GYhO3CZm8jP625aTS9fRGLC/QiMuOATGQgMUVdJ0hB9m1UztRxZ9VsAmXWBRI99e77XsqT4uQvuhJ/HVzQ2nc0IiFTftM2eGWv2Gq3R8C32w0FeRV0daLXx+bHic1dv3Afaj8cj9X9+6j8AEChMqqtPk4lW09r/OXb95EpM4uDGG645/FH9fwgBFlR4N+CnYpQUu+cR2lj/0t/S5JanjYaxUk6p9LkJVM0/fGpfW8Y0ZfEsqp5omnc07D8QF+4jxgIm2erZb6GVlS7Pt6mZ2iuIq2wLyLbsXDl3EPT+7h5HACBiDAKuOf8Ta/oH4jSPN29K0fVDFSwN+CrG5/aoZ3dQ2bcGI4VUACa9K5KmibVYDQFLQt8vJAzQGOfkmceDNk8/VrSZTqhH3ewMjaRd6SZXLT1QqV9k6LbaqJ5SO/GsIJlyv4J4Sem1GCg0ig5TBd0O3QYN+CdA6/DR+/T//C299sIO/m9UAKZa8s6qJ75xWa7lUnscY7p3vXAMrq5rzUq8rz6TJWXN4NG61TeSaiXQX1UUmbsutNDMnienJ2KKAb58dl4aMqpsHt79R2P1UlrQOfwI6D3Wy8ecbGRwbDPeZAZk9M227YKv0TWTEv7GhPrpUXlyVjkOyWt2qiGVy5bPHLaxesSjtgvSJq5vV1XgQgYHh8bI6E3JUqsZtdnn8wXdDJZxaMVbUNOBHhFd/WltrTo7Iw0E+k3OguJr4GL6h4egyfLnWxdulbTJa3aqEpGtyVl/r4e4vnsXKlmbbs4DYXHy5r25WUAmFBklG+uGJv4DuH8JNOwq3XyojjgR8EXkYuBx43xhztsXjArQDlwHDwM3GmNeceO6sJJ6yeqZBcJz2pln4XRP/L4mriY9weKSTKldtl6euq/E48tyFZvdBZdWnvhIWpC8qFoUGlg7/e6hldwEv/ivXeSwnOTXC/z7wT8APbB7/AnBG+OsPgH8O/1swiaka7+wa1nxYG0q7hLtNpm5VYMEYphgTamI2PApLvpK3F3iqtM3qFYuSShcBhsb8dHT3lNyLPt1as4lv7GsuaC6qzqZlLa7QwHqkv6HBy7YZ0wl+sAvXD87lut/Nf1vtdJP3KsSxSVsRmQ88YzPC/x7wojHm0fDtA8AyY4x1dyecmbRN7COSmIePbyIFy+fOsa2qAeIWAAGYN1TD4Ef3FaTD4oI1nZappcioN3L1aKJS7ACZahIaKLsJ6pJlMYG7ocGb3IYbOH3G6XRc1ZG3XbFrgFiKr/9cpZq0LdSVts1A7CvjSPi+OCJyi4h0iUjX0aPZrSvauf1Glj/0e5zz/bNZ89KdDAZHole5JhoXob3eG73demyAqqD1lYeeoGHj+x+y7/C7/M3BRXz0q/t46922guXJ7erDI/cP2LT9LcU8vtWVz5Fg//Wt+2zPdFSBXboOXPFpw20WwR7g18d/zYbdGxx9+o7uHpZuep4Fazptq7lK8fWfT0U1aWuMeRB4EEIj/In+fOf2G2n7aD+jVZkfVmwax6qqBkKVNWcc+32+1n9t0s8X6kKddLnqdGmQUpM40RoZ9dtdNZtprb1yUCS90/FVCI4BqZfS3PYf2xxL7ZTUcpVFpFAj/B5gXsztueH7HNU+uNd+PU8b0eqacLdJ39AILx938fr560IdBm9+g6+ft5PXProx6WcLOTloN+qNBEWr3jHlNHmZrnLHqu+RKoDFq2Dd0dD8lbhTBpRgMBAq3bynITShm4NyWq6ykAo1wt8BfE1EHiM0WTuYKn+frT73xCZdPcaEqmtSVNSs7XidH+1+Jyl/HlsaWCipygsT683LbfIy3an5RPrlqDy4/H64/H6u270h2ks/UfTDwASg6yH48GDWpZvpXg9uEa65QMtxEzlVlvkosAyYJSJHgLsBD4Ax5gHgx4RKMg8SKsv8MyeeN1FjIEBvhumc2qpa1v3hupTrtnZ091gGe4DaKVUFfTFlUnJWzvXm6VokNOupe1FYe9Faunq7+PXxX8c/YNWM7fC/h0qksyhdTvd6CBjD43t6WHJaQ9m+J7LhSErHGHOjMabJGOMxxsw1xjxkjHkgHOwxIV81xpxujDnHGJOXngmtdecltRfGmLgvr7uGTZ/axM//+OdpF+nevPOA7UVXhcwZR/KVPQMjGE6WnJXjgh127Nodg566F5uOqzq4ftH1uML9plzGcL1VMzaAJ/9LVourpHo9RIyMB7hty16Wbnq+ot4rqZRdL53O7TfSPriXPrebxkCA1rrz8F37aFb7YFcKCaFTxl9vvCyr3ztRWnIWEttfp1yXJyxLbXVpNsh8Oc7YM93YNhnpolglle5WVC8d37WPknrcnrlUp42FzBmXW+uEbJVzyqqsTZkGY0NJd3dOq6W93ktflZvGvmdpfXFayh5TiZU5AyPj1HjcfPv689I22ivl3lJO0hWvUli9YhF2tR+FzBnblZa5RFiwplNPWVVxu/w7SXd1TqulbVYDvZ4qjEiozfhvnqTzUKftr0l3tXm6FE+lDZCsaMBPYWVLM3980alJQT+SM4698COfQdfuxRwwpmJz+qqELF4VKtuM0V7vTSqhHhWh/bV2219jF7B7BkYsL8hLNMdbU7D3bLHSgJ/GhpXn8O3rz7O86nP19n1xE6mrt+/LywsosQY/67V2lZosl98PV/9LqAQa+75VfUN9cbdjA7QrxbUWiSlWtyt+2xqPm0vOnF3xxQ9lN2lbKHa9a+prPXSvW57X507XV0epovbMHSzve9ayb5VLXBhjaJzWyNKGP+GxF2anHbnbSZzYt8vzl1vxQzH00ik7VsE+1f1OStdXR6lYRZfGuPx+Wn/nWqotBptBE8Rg6B3q5Zm3N9E2/a+4wvVy3DZukejZdioBY6Lp13JcNyIbGvBLTEd3D/1DJ5Lu11p0ZaVYr+HwLbuXtk/fR9O0JgSJ1uzHGnW5+G+fmMZnvY/EBf2gMRze5OOVNZ9J21IjNtWpAyUN+FnzplhcJF+jqI7uHlZv28fIeHKLKr2MXFlJVdky2XwLfey6dhf7b9qPXWo5KMK9s+u51PtI9L7YAH3jH8yz+rE4kRF8ufebyoQG/Cy1XXEWHpf16CJfo6jNOw8kLXIS8cKvsmsnrcpbqaQxGqc12j426nLx3YY6fuD5+6QKuR/tfift745U50Q+/CJnBYkNCCuBBvwsrWxpZvN159rmEfMxikr1Ji22N7AqDqWSxmg9v9Uypx/RV+XmU+43+cHvvw0Ql6ZKJbE6B5Jz+5VEA36WYi/xtuN0EE71Ji22N7AqDqWSxvAt9NE2/ypcNkFfgHPnz+MbR9vZ8cLajCt3qj0uOvf3Fm1aq9DKrrVCPsX2chFIO7pwOgjbrV3rcUvRvYFVcSilttm+ZffC9oOhRYxiL8oyhmA4DdNb5aa//lWqRk/Hf7zF8vfEvjdTVc1V4lmxjvAzFFvtAOmDfT5GUZE0UuyEcX2th83XnluUb2BVHFa2NPPKms/w7evPA+D2Iu4g6bv2Uf7yA6Fp3I8YExrxJ1TinHAJU2fvTPrZZm8Nzd6atO/NiEo8K9YRfoYyWWEHQqOLfI6itIGYykZi47FIYQFQdK+nDR/exyPH/55Pud7k3AXWVTjiGYi/TegM+PYtezN6jmJMaxWCjvAzlMnpX7O3JlofXGxvIlXZirk8M5G31sOfjn+D1vG/5pN+60HWHL+fl6fcGq3PN4Q+uOxG7d4aj+3yoJVER/gZSrfCjgCXnDmbpZueL/pcqao8pVKeCaG1igB2BC/mwqNHONbYzYmYEujqYJBPDw/z5VNr6at6mmb//2Ls4xsBH6tXLEpa3LzG46btisIuR1qsNOBnyOqFFMsAj+/pmfApcyZLFyqVK7sBS12Np+gGKYMjJydafzF4A1VmEVNn78TtOUajP8Cnh4d5avop0Ynd4x4/eP+NDbvHWHvRWqA0Jqkngwb8DEVeMF/fus9y8RO3SMpTZqsXYCnlVVVpsxqweFzC0JifgXCALZbXX+KHk/94C/7jLRyq/mNcGJbPnZPUWhmBLQe20PKJFla2+DLef7sVtMr1g0Jz+BOwsqWZb60617Ku2W4FrMibyKqXSSnlVVVpS2yx3eyt4ZTqKsYD8a/bYnj92V07MFoTuhrXrrUywMZX7sn4eRL7DA2MjHMsvFxisfQccpoG/AmyeuNEbtuxC+qllFdVpS9SnhkpLBiwqVGfzNdfqhYItV9YDwiNNhO5AIOBYXjmjoyeK13lXTF8+DlNUzpZsCuNTJXjTxRJ71jlVSuxPlgVXrG9/hJTnMktEFbBO7tp/eWjrJk9M6k+P2J537O0PtKN76YXUj5fJh9s5Tb40hG+Q2JH/pmI5AhL4bJ3VZ6K7fWXUYrz8vvx/d6N1Nr13Ymsjxt8n85HLkn5fJl8sJXb4EsDvoMip8zpxI5arNJD5TZRpIpTsb3+Mk5xXn4/66YvxpOi2dqoy0X7eF/K9M4lZ85OuT/lOPjSlE6BNSfM/uuVs2oyFdPrbyIpJt+1j8KL36T94HZ6q9yW6Z2+Kjd0PQSnXhRaSD1Bqpbiie/TcqEj/Dyor7VeHKW+1qNX4SplY6IpJt+ye9nV+AWabCZxo5O7T30V9m9NetzujEKgbN+nGvDz4O4vnoXHHT/i8LiFu7941iTtkVLFL5Jiim0OWO1JE6Iuv59WTyPVwfhV4KqCQUZcwuL581jeNIvOF76R9KOlslaAkzTg58HKlmY2X3tuXG5UO1oqlZkT/pPB+9jwOLdv2cvajtdtt/fd9AJtrk9EO2zW+QOICANuNyYyiTtjCp0vfjPu54pt0roQxG4tycm2ZMkS09XVNdm7oZQqoKWbnrfM4wvw7evPSz1o+m9zYGyI5XPn0OtJnp5sChh2ffmNuPvKsbWJiOwxxiyxesyRSVsR+TzQDriBfzXGbEp4/GZgMxC5bO2fjDH/6sRzK6XKh11e3RAq20wZjC//DnT8te2VuH0W+YximrQuhJwDvoi4ge8CnwOOAK+KyA5jzFsJm24xxnwt1+crFeU4clAqF7ErxrlFCBiTVA2Tqitt2ougwpU4ja/eTW9VcmhrDCbdVXGcyOFfCBw0xhwyxowBjwFXOvB7S1Zij45y7cuhVKYSV4yL9J5KfG+kyp9nNJm6eBWtp19LdcIyoNVBQ+vCq7Lc+/LhRMBvBt6NuX0kfF+ia0Rkv4hsFxHLZWxE5BYR6RKRrqNH7Wtki502RVOlrKO7h/Pu2cX8NZ3MX9NJy/pdOQ9WUvWtSXxvWAWliazb7Ft2L20LrqIpYBBjaAoYrqyeR/uhJ1n8/bNZ/vDZSRO4laJQVTpPA/ONMYuBnwCPWG1kjHnQGLPEGLNk9uzUV8EVM22KpkpVR3cPq7fti7ZMhlClzOrt+3IK+qkWD4KT743NOw9glXmZNqVqQilR37J72fXlN9h/8xu0LryKp0bfpdctoaodt9B2+MmKDPpOBPweIHbEPpeTk7MAGGM+NMacCN/8V+ACB563aFVifa8qD5t3HmA8mFy5Nx4wWZ+hdnT3YN3m7KS6cO293aAodlGUiWo/9CSjrvg9GHUJ7YeezPp3lionAv6rwBkiskBEpgA3ADtiNxCRppibVwC/dOB5i1Yl1veq8pDqLDTbM9TNOw+Qrvh7aMxPR3dPXgZLVtU50fstrsAtZzkHfGOMH/gasJNQIN9qjHlTRNaLyBXhzW4VkTdFZB9wK3Bzrs9bzIqtKZVSmUoVWLMNuunSOXDyDCIfgyW76pxGfwCevjVt0O/o7mHppudZsKaTpZueL+niC0fq8I0xPwZ+nHDfupjv7wLucuK5SkWl1feq8rB6xSJWb9uXlNaZyKRpokgJZjrvDYxE3zNOljS3LryKtsPxaZ3qYJDWYwMwPgLPrbdsrgbJPfqLZRnIbGm3TKVUVCSIte14MzpxW1/r4e4vnpV1gMsk2MPJMwinB0u+ZfcC0H5wO31Vbhr9AVqPDeAbGg5tMHjE9mdTVdxpwFdKlTynA25zioupIvI9x+Vbdi++7idh8N3kB8UFbV6omwuXrosb7ZdbxZ02T1NK5dXqFYuSuse6JHTmUNA5rkvXgSd+HqJzWi3Lmz/J4vlzWT49QOeuO+Jy+uVWcacjfKVU/iVkddwiOaWJshIZuT+3HgaP0DmtlrZZ9Yy6QuPeXk8VbTPr4IVv4Fu8io7uHoZO+JN+TSlX3OkIXymVV1a1/ePB7Ov6c7J4Fdz+BrQN0F5fFw32EaMuF+3TqqKTtQMJ9f/1tZ6SrrjTgK+UyqtizYPbdtWsctu2gjg2PM7mnQdKtjRTA75SKq+KNQ/eGLCuHmoMmJQfRqXcDFEDvlIqa5lclFSsV563nn510tKI1cEgrf39/J/qVq5wvWz7s6XaDFEDvlIqK1ZtwG/bsjepu2ZWa9UWQKir5tUnu2qO+2n7oB/f0DCNHOU+z7+mDPqTnZLKxuT/ryulSlKqPLdVyiNxrdpiSItEu2oeg11H3gNg+dw5LJ4/jyvnzeIP6p+w/dnJTkllQwO+UiorqUa4iSmPol8jIlqm2UCvpyq6+Pmm2R4uXfpyUaaksqEBXymVlXQj3NgPhGKt1Imqm0t7vTepTBMRftH/DDdccrQsmiHqhVdKqaysXrEorrFYotgPBLu1aosmLXLpOvpeW2/78Cv9/8Yra3YVcIfyQ0f4SqmsWE3GRiSmPIq1Uidq8Soap3htH+77+L2y6J2vAV8plbWVLc20XXFWXNC3uhq1FNaIaL3IvoN7pr3zi52mdJRSWUvsFw8wOm694kixrxHhW+ij+/1uthzYEnd/pr3zS4GO8JVSWSv66psJWnvRWjZ9ahNN4/6k2nwgZe/8UqAjfKVU1oq++iYLvoU+fE/dadk732AYbJvL3WN/QteMz+W8Gleh6QhfKZW1Yu2TkzOL3vkAAnj5iM2eB7ng+E+4fcte1na8Xvj9y5IGfKXUhMT2zxk64U9a3KSoqm+ytXgVfPEfQaw7ak4RP39btRUD/Gj3O5N+xXCmNOArpTKW2D9nYGQczCSsXlUIi1eBsZ6ABpgjHwKhtV1KZc5Cc/hKqYxZTdKOBw21U6roXrd8kvYqj+rmWq+DC7xnZp78vkTmLHSEr5TKWDlO0qZ06TpwnbzGoHNabbS52s2nTuXCuseA0pmz0BG+Uipjdi0S6mo8LN30PO8NjDDHW1Ny1Su2IjX3z95Jp2uUtlkN0X47v/W4+W1TN5+edoirP1UaF2TpCF8plTGrFgkelzA05o/ri18MrY8ds3gV3HmY9pkzLZurdc8YxDP4wOTs2wRpwFdKZcyqRcIp1VWMJywXWMoXX9nps4uWImw8/GRB9yVbmtJRSk1IYouEBWs6Lbcrt7x+YxB6ras0GRToPNSJb6GvsDs1QTrCV0rlpGwvvkrQuvAqMNYLnyPCXS/cl3Jt32KgAV8plZOib33sEN+ye7l+6jzboB90Hyv6OQxHAr6IfF5EDojIQRFZY/H4VBHZEn785yIy34nnVUpNvlJofeyUtTc+i7eq1vIxM+6Nfl+scxg55/BFxA18F/gccAR4VUR2GGPeitnsK8AxY8zviMgNwH3A9bk+t1KqOBR762MnrfnPd9P2szZGA6PR+6YGDXcfO0jLlFv5B/8qdgQvLso5DCcmbS8EDhpjDgGIyGPAlUBswL8SaAt/vx34JxERY+wSYkopVZwiE7Ptr7XTN9TLdH8Qlxi+8YmZNDYE+Mv+H8IA7JnxucndUQtOpHSagdhrj4+E77PcxhjjBwaBmQnbICK3iEiXiHQdPXrUgV1TSinn+Rb62HXtLjZ+bBhzwYDbjRGh11PFxtkz+CPvj4pyDqOoJm2NMQ8aY5YYY5bMnj17sndHKaVSap8aSLoYa9Tl4nsN01jZ861J2it7TgT8HmBezO254fsstxGRKqAO+NCB51ZKqUnTV2VdmN9X5Yauhwu8N+k5EfBfBc4QkQUiMgW4AdiRsM0O4Kbw99cCz2v+XilV6hqneK3v9wcAU3SLnucc8MM5+a8BO4FfAluNMW+KyHoRuSK82UPATBE5CNwBJJVuKqVUqWm96C6qg/E986OLnkNo0fMiIsU60F6yZInp6uqa7N1QSqmUHv6fV/NY4Jf0Vblp9Af49PAwL9XWRm+3fua/F7TlgojsMcYssXqsqCZtlVKqlHR097D50C088W4/+3/zLq3HBnhq+in0eqqiVTvf+PdvcM/z/zbZuwpowFdKqaxFVgD7u/EvM2ym0F7vTaraCbgCPHN4c1G0WtCAr5RSWYpcTbsjeDFrxv/ctmpn1B1kxwtrC7lrljTgK6VUlmI7gu4IXhyuzrEgwjszfl6gvbKnAV8ppbK0esUiJOb2l/oDtt0036+a/HA7+XuglFIlamVLM3980anR210D1+BNKNOMsB39F5AGfKWUysGGlefwnevPw1vjYUfwYm774GPr2vwh/yTt4Um6xKFSSuUorj30/v9O9U++TnvdKSdr8Qc/xve5ye+towFfKaWctHgVPsD33HoYPAJ1c+Fz34LFqyZ7zzTgK6WU4xavig/w+7fCt88++QFw6bpJ+QDQgK+UUg7r6O5h884DvDcwwk2n/IK15gGqIitkDb4LT98a+r7AQV8nbZVSykEd3T3c9cTr9AyMYIA/H/vhyWAfMT4yKY3VNOArpZSDIu0WIubIB9YbDr5rfX8eacBXSikHJS5e/p6ZZb/xM3fkeW/iacBXSikHxbZbAPgH/yqCdl3oux4u6CIpGvCVUspBq1csosZzsonajuDFce0XOqfVsnzuHBbPn8fyuU10/rRwuXwN+Eop5aCVLc1svPocmr01CNDsrWGktgkIBfu2WQ1x/fLbag2dhzoLsm+64pVSSuXb/q3wxC0sn9tErye5Gr5uSh0v3/hyXDnnHG8Nq1csOnkFb4ZSrXildfhKKZVvi1fBO7vp+2CX5cODJwa5aes/84vXF0QrfHoGRrjridcBJhz07WhKRymlCqCj+etUB2qtHxTY99G/xJVzAoyMB9i884Bj+6ABXyml8ixyMVb/b6+wa5dPwD3GhXWPJd2fWOaZCw34SimVZ5GLsfzHWzC2o3zhw9mvJd2dWOaZCw34SimVZ7Gj9BO//aLtqlhHqyTudo3HzeoVixzbDw34SimVZ7GjdP/xFmZYL4rFJwLElXNuvPocxyZsQat0lFIq7y45czY/2v0OkXF9829bGGt8jVHXyTF3dTDI7R+P4bvsg7x10dQRvlJK5VFHdw+P7+khNonz6uANfJ4/oskfRIyhadxP2wf9+I69H2qdnKd2CzrCV0qpPErsnglggOd/ey2vTH0puWtmpHVyHkb5GvCVUiqP7Moq3xsYgeojAGxo8LJtxnSChNIu1x3/mLV52BdN6SilVB7ZlVXO8dZA3Vw2NHjZMmM6QREQISjClhmnsGH3Bsf3JaeALyINIvITEfn/w//W22wXEJG94a8duTynUkqVksTumRHDY35ePf2/sm3GdJD4ckxE2PYf2xzfl1xH+GuA54wxZwDPhW9bGTHGnBf+uiLH51RKqZIR6Z7prfHE3X9seJw/ffW00MjeQtAEHe+imWvAvxJ4JPz9I8DKHH+fUkqVnZUtzUybmjxlOjIeAGMfhtte/qajQT/XgP9JY0xv+Ps+4JM221WLSJeI7BaRlXa/TERuCW/XdfTo0Rx3TSmliofd5O3YsQttf2bUjNO+e6Nj+5A24IvI/xaRNyy+rozdzoQa69s11z8t3J/5/wG+IyKnW21kjHnQGLPEGLNk9uzZEz0WpZQqWnaTt7NO3Mj1i663bbfQNzbg2D6kDfjGmM8aY862+HoK+K2INAGE/33f5nf0hP89BLwItDh2BEopVQKsJm9rPG4uOXM2z774n2nyByx/rtHm/mzkmtLZAdwU/v4m4KnEDUSkXkSmhr+fBSwF3srxeZVSqqRYLX14zQXNPL6nh56BEb7UH6A6GN9kpzoYpPVEcoVPtnK98GoTsFVEvgK8DawCEJElwF8aY/4c+D3geyISuaZgkzFGA75SquKsbGmOa4a2dNPz0atwuwau4S5+yAMNp9BX5abRH6D1+DC+z2527PlzCvjGmA+BSy3u7wL+PPz9z4BzcnkepZQqR7ETuTuCF8MAPPzxVubIh7i8c+Gzmx1tsaCtFZRSapLM8dbQkxD0d4xdTLO3hldu/4zjz6etFZRSapLYTeQ6uehJLB3hK6XUJInk8zfvPMB7AyPM8dawesUiRxc9iaUBXymlJlHiRG4+aUpHKaUqhAZ8pZSqEBrwlVKqQmjAV0qpCqEBXymlKoQYmw5tk01EjhJq15CtWcAHDu1OqajEY4bKPO5KPGaozOOe6DGfZoyxbDdctAE/VyLSFW7JXDEq8ZihMo+7Eo8ZKvO4nTxmTekopVSF0ICvlFIVopwD/oOTvQOToBKPGSrzuCvxmKEyj9uxYy7bHL5SSql45TzCV0opFUMDvlJKVYiSDvgi8nkROSAiB0VkjcXjU0VkS/jxn4vI/EnYTcdlcNx3iMhbIrJfRJ4TkdMmYz+dlO6YY7a7RkRMeJnNkpfJcYvIqvDf+00R+f8KvY9Oy+D1faqIvCAi3eHX+GWTsZ9OEpGHReR9EXnD5nERkX8M/5/sF5Hzs3oiY0xJfgFu4NfAQmAKsA/4Twnb/DXwQPj7G4Atk73fBTruS4Da8Pd/VerHnckxh7ebDrwE7AaWTPZ+F+hvfQbQDdSHb39isve7AMf8IPBX4e//E/Cbyd5vB47708D5wBs2j18GPAsIcBHw82yep5RH+BcCB40xh4wxY8BjwJUJ21wJPBL+fjtwqYhIAfcxH9IetzHmBWPMcPjmbmBugffRaZn8rQHuBe4DRgu5c3mUyXH/BfBdY8wxAGPM+wXeR6dlcswGmBH+vg54r4D7lxfGmJeA/hSbXAn8wITsBrwi0jTR5ynlgN8MvBtz+0j4PsttjDF+YBCYWZC9y59MjjvWVwiNDEpZ2mMOn+LOM8Z0FnLH8iyTv/XvAr8rIq+IyG4R+XzB9i4/MjnmNuBLInIE+DHwXwuza5Nqou97S7riVRkTkS8BS4A/mux9yScRcQH3AzdP8q5MhipCaZ1lhM7kXhKRc4wxA5O5U3l2I/B9Y8y3ROQPgX8TkbONMcHJ3rFiV8oj/B5gXsztueH7LLcRkSpCp38fFmTv8ieT40ZEPgt8A7jCGHOiQPuWL+mOeTpwNvCiiPyGUI5zRxlM3Gbytz4C7DDGjBtjDgP/QegDoFRlcsxfAbYCGGP+D1BNqMFYOcvofZ9OKQf8V4EzRGSBiEwhNCm7I2GbHcBN4e+vBZ434RmQEpb2uEWkBfgeoWBf6jldSHPMxphBY8wsY8x8Y8x8QvMWVxhjuiZndx2TyWu8g9DoHhGZRSjFc6iA++i0TI75HeBSABH5PUIB/2hB97LwdgB/Gq7WuQgYNMb0TvSXlGxKxxjjF5GvATsJzew/bIx5U0TWA13GmB3AQ4RO9w4SmhC5YfL22BkZHvdm4BRgW3iO+h1jzBWTttM5yvCYy06Gx70TWC4ibwEBYLUxpmTPYjM85q8D/yIitxOawL251AdyIvIooQ/uWeG5ibsBD4Ax5gFCcxWXAQeBYeDPsnqeEv9/UkoplaFSTukopZSaAA34SilVITTgK6VUhdCAr5RSFUIDvlJKVQgN+EopVSE04CulVIX4v1ttcP3NLzkJAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Losses:"
      ],
      "metadata": {
        "id": "dkWca9S4nSXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(vLoss_training)\n",
        "plt.plot(vLoss_test)"
      ],
      "metadata": {
        "id": "_xgeWHjEnU2r",
        "outputId": "14796f07-921c-4ee0-d61c-942a0e30d817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff2b240b160>]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FubOpmdZdyoA"
      },
      "source": [
        "# Report\n",
        "Write your test results and utilized hyperparameter parameter values in this section. Briefly discuss how your algorithm works; and why it works as it is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftGFf6OAdyoA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}